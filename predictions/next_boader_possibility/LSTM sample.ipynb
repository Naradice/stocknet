{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8c6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fec1970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6e2e836df8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b5dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkDataSet(data_size, data_length=50, freq=60., noise=0.02):\n",
    "    \"\"\"\n",
    "    params\n",
    "      data_size : データセットサイズ\n",
    "      data_length : 各データの時系列長\n",
    "      freq : 周波数\n",
    "      noise : ノイズの振幅\n",
    "    returns\n",
    "      train_x : トレーニングデータ（t=1,2,...,size-1の値)\n",
    "      train_t : トレーニングデータのラベル（t=sizeの値）\n",
    "    \"\"\"\n",
    "    train_x = []\n",
    "    train_t = []\n",
    "\n",
    "    for offset in range(data_size):\n",
    "        train_x.append([[math.sin(2 * math.pi * (offset + i) / freq) + np.random.normal(loc=0.0, scale=noise)] for i in range(data_length)])\n",
    "        train_t.append([math.sin(2 * math.pi * (offset + data_length) / freq)])\n",
    "\n",
    "    return train_x, train_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517738ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outputDim):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size = inputDim,\n",
    "                            hidden_size = hiddenDim,\n",
    "                            batch_first = True)\n",
    "        self.output_layer = nn.Linear(hiddenDim, outputDim)\n",
    "    \n",
    "    def forward(self, inputs, hidden0=None):\n",
    "        output, (hidden, cell) = self.rnn(inputs, hidden0) #LSTM層\n",
    "        output = self.output_layer(output[:, -1, :]) #全結合層\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be40a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 100 #traning dataのデータ数\n",
    "epochs_num = 1000 #traningのepoch回数\n",
    "hidden_size = 5 #LSTMの隠れ層の次元数\n",
    "\n",
    "train_x, train_t = mkDataSet(training_size) #Datasetの作成\n",
    "\n",
    "model = Predictor(1, hidden_size, 1) #modelの宣言\n",
    "\n",
    "criterion = nn.MSELoss() #評価関数の宣言\n",
    "optimizer = SGD(model.parameters(), lr=0.01) #最適化関数の宣言}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9c297c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.016904171041831777],\n",
       " [0.11425490580939804],\n",
       " [0.23673697509116806],\n",
       " [0.30678324559886216],\n",
       " [0.42170992902598614],\n",
       " [0.4849151372922008],\n",
       " [0.6138813994044291],\n",
       " [0.6647565036512278],\n",
       " [0.7385089633880186],\n",
       " [0.7714483018814303],\n",
       " [0.8643916805161332],\n",
       " [0.8951319238568471],\n",
       " [0.9585758684302778],\n",
       " [0.9495062886244593],\n",
       " [0.9682634745332497],\n",
       " [1.0168653912399674],\n",
       " [0.9815840582171123],\n",
       " [0.9640332460312924],\n",
       " [0.910831601004271],\n",
       " [0.913327372387491],\n",
       " [0.908306035412673],\n",
       " [0.8342649530419809],\n",
       " [0.7372705494503413],\n",
       " [0.6700462548532169],\n",
       " [0.5573641189928182],\n",
       " [0.5077702900187906],\n",
       " [0.42199797548827894],\n",
       " [0.3016608674221748],\n",
       " [0.21229314515366754],\n",
       " [0.12998816854901216],\n",
       " [-0.004138231129648321],\n",
       " [-0.06840103788146149],\n",
       " [-0.22023315149847753],\n",
       " [-0.30239089047376383],\n",
       " [-0.4116324729639558],\n",
       " [-0.5305192403891544],\n",
       " [-0.5791263287136266],\n",
       " [-0.679880543915686],\n",
       " [-0.7661426527211158],\n",
       " [-0.8187674699087375],\n",
       " [-0.8776936363888214],\n",
       " [-0.906060172101305],\n",
       " [-0.9525039245008813],\n",
       " [-1.002975052970258],\n",
       " [-0.9857124854907608],\n",
       " [-0.997772412388034],\n",
       " [-0.9659002588919369],\n",
       " [-0.9604303533180345],\n",
       " [-0.9722952489000012],\n",
       " [-0.8916026782838797]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a49e977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss: 33.072, training_accuracy: 0.09000\n",
      "2 loss: 20.486, training_accuracy: 0.11000\n",
      "3 loss: 10.432, training_accuracy: 0.14000\n",
      "4 loss: 6.549, training_accuracy: 0.18000\n",
      "5 loss: 5.232, training_accuracy: 0.19000\n",
      "6 loss: 4.595, training_accuracy: 0.19000\n",
      "7 loss: 4.163, training_accuracy: 0.19000\n",
      "8 loss: 3.816, training_accuracy: 0.18000\n",
      "9 loss: 3.516, training_accuracy: 0.20000\n",
      "10 loss: 3.247, training_accuracy: 0.23000\n",
      "11 loss: 3.001, training_accuracy: 0.26000\n",
      "12 loss: 2.773, training_accuracy: 0.26000\n",
      "13 loss: 2.559, training_accuracy: 0.28000\n",
      "14 loss: 2.356, training_accuracy: 0.31000\n",
      "15 loss: 2.163, training_accuracy: 0.33000\n",
      "16 loss: 1.977, training_accuracy: 0.34000\n",
      "17 loss: 1.798, training_accuracy: 0.35000\n",
      "18 loss: 1.626, training_accuracy: 0.37000\n",
      "19 loss: 1.461, training_accuracy: 0.40000\n",
      "20 loss: 1.303, training_accuracy: 0.43000\n",
      "21 loss: 1.153, training_accuracy: 0.48000\n",
      "22 loss: 1.014, training_accuracy: 0.50000\n",
      "23 loss: 0.888, training_accuracy: 0.55000\n",
      "24 loss: 0.775, training_accuracy: 0.67000\n",
      "25 loss: 0.677, training_accuracy: 0.71000\n",
      "26 loss: 0.594, training_accuracy: 0.76000\n",
      "27 loss: 0.526, training_accuracy: 0.84000\n",
      "28 loss: 0.473, training_accuracy: 0.87000\n",
      "29 loss: 0.431, training_accuracy: 0.89000\n",
      "30 loss: 0.399, training_accuracy: 0.89000\n",
      "31 loss: 0.374, training_accuracy: 0.90000\n",
      "32 loss: 0.356, training_accuracy: 0.91000\n",
      "33 loss: 0.341, training_accuracy: 0.94000\n",
      "34 loss: 0.330, training_accuracy: 0.94000\n",
      "35 loss: 0.320, training_accuracy: 0.95000\n",
      "36 loss: 0.312, training_accuracy: 0.95000\n",
      "37 loss: 0.305, training_accuracy: 0.95000\n",
      "38 loss: 0.299, training_accuracy: 0.95000\n",
      "39 loss: 0.293, training_accuracy: 0.95000\n",
      "40 loss: 0.288, training_accuracy: 0.96000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-43f24ef82bc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs_num):\n",
    "    # training\n",
    "    running_loss = 0.0\n",
    "    training_accuracy = 0.0\n",
    "    for i in range(training_size):\n",
    "        optimizer.zero_grad()\n",
    "        data = torch.tensor([train_x[i]])\n",
    "        label = torch.tensor([train_t[i]])\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data.item()\n",
    "        training_accuracy += np.sum(np.abs((output.data - label.data).numpy()) < 0.1) #outputとlabelの誤差が0.1以内なら正しいとみなす。\n",
    "    training_accuracy /= training_size\n",
    "    print('%d loss: %.3f, training_accuracy: %.5f' % (epoch + 1, running_loss, training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6d66485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkRandomBatch(train_x, train_t, batch_size=10):\n",
    "    \"\"\"\n",
    "    train_x, train_tを受け取ってbatch_x, batch_tを返す。\n",
    "    \"\"\"\n",
    "    batch_x = []\n",
    "    batch_t = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        idx = np.random.randint(0, len(train_x) - 1)\n",
    "        batch_x.append(train_x[idx])\n",
    "        batch_t.append(train_t[idx])\n",
    "    \n",
    "    return torch.tensor(batch_x), torch.tensor(batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c567fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss: 0.043, training_accuracy: 0.81000\n",
      "2 loss: 0.044, training_accuracy: 0.90000\n",
      "3 loss: 0.041, training_accuracy: 0.86000\n",
      "4 loss: 0.035, training_accuracy: 0.95000\n",
      "5 loss: 0.035, training_accuracy: 0.94000\n",
      "6 loss: 0.036, training_accuracy: 0.94000\n",
      "7 loss: 0.032, training_accuracy: 0.93000\n",
      "8 loss: 0.033, training_accuracy: 0.96000\n",
      "9 loss: 0.031, training_accuracy: 0.96000\n",
      "10 loss: 0.036, training_accuracy: 0.94000\n",
      "11 loss: 0.032, training_accuracy: 0.96000\n",
      "12 loss: 0.035, training_accuracy: 0.92000\n",
      "13 loss: 0.030, training_accuracy: 0.98000\n",
      "14 loss: 0.031, training_accuracy: 0.96000\n",
      "15 loss: 0.028, training_accuracy: 0.97000\n",
      "16 loss: 0.033, training_accuracy: 0.91000\n",
      "17 loss: 0.031, training_accuracy: 0.97000\n",
      "18 loss: 0.031, training_accuracy: 0.95000\n",
      "19 loss: 0.030, training_accuracy: 0.97000\n",
      "20 loss: 0.027, training_accuracy: 0.96000\n",
      "21 loss: 0.028, training_accuracy: 0.97000\n",
      "22 loss: 0.028, training_accuracy: 0.95000\n",
      "23 loss: 0.031, training_accuracy: 0.95000\n",
      "24 loss: 0.028, training_accuracy: 0.95000\n",
      "25 loss: 0.038, training_accuracy: 0.92000\n",
      "26 loss: 0.034, training_accuracy: 0.96000\n",
      "27 loss: 0.030, training_accuracy: 0.95000\n",
      "28 loss: 0.032, training_accuracy: 0.96000\n",
      "29 loss: 0.030, training_accuracy: 0.95000\n",
      "30 loss: 0.036, training_accuracy: 0.94000\n",
      "31 loss: 0.034, training_accuracy: 0.94000\n",
      "32 loss: 0.029, training_accuracy: 0.95000\n",
      "33 loss: 0.032, training_accuracy: 0.93000\n",
      "34 loss: 0.029, training_accuracy: 0.94000\n",
      "35 loss: 0.030, training_accuracy: 0.95000\n",
      "36 loss: 0.030, training_accuracy: 0.93000\n",
      "37 loss: 0.037, training_accuracy: 0.92000\n",
      "38 loss: 0.030, training_accuracy: 0.94000\n",
      "39 loss: 0.027, training_accuracy: 0.96000\n",
      "40 loss: 0.032, training_accuracy: 0.95000\n",
      "41 loss: 0.030, training_accuracy: 0.95000\n",
      "42 loss: 0.026, training_accuracy: 0.97000\n",
      "43 loss: 0.030, training_accuracy: 0.95000\n",
      "44 loss: 0.032, training_accuracy: 0.94000\n",
      "45 loss: 0.029, training_accuracy: 0.94000\n",
      "46 loss: 0.037, training_accuracy: 0.93000\n",
      "47 loss: 0.027, training_accuracy: 0.96000\n",
      "48 loss: 0.033, training_accuracy: 0.95000\n",
      "49 loss: 0.027, training_accuracy: 0.96000\n",
      "50 loss: 0.030, training_accuracy: 0.95000\n",
      "51 loss: 0.032, training_accuracy: 0.96000\n",
      "52 loss: 0.029, training_accuracy: 0.97000\n",
      "53 loss: 0.028, training_accuracy: 0.98000\n",
      "54 loss: 0.027, training_accuracy: 0.96000\n",
      "55 loss: 0.030, training_accuracy: 0.93000\n",
      "56 loss: 0.028, training_accuracy: 0.97000\n",
      "57 loss: 0.027, training_accuracy: 0.96000\n",
      "58 loss: 0.026, training_accuracy: 0.98000\n",
      "59 loss: 0.033, training_accuracy: 0.94000\n",
      "60 loss: 0.033, training_accuracy: 0.97000\n",
      "61 loss: 0.028, training_accuracy: 0.98000\n",
      "62 loss: 0.027, training_accuracy: 0.98000\n",
      "63 loss: 0.032, training_accuracy: 0.97000\n",
      "64 loss: 0.022, training_accuracy: 0.98000\n",
      "65 loss: 0.030, training_accuracy: 0.97000\n",
      "66 loss: 0.032, training_accuracy: 0.96000\n",
      "67 loss: 0.026, training_accuracy: 0.95000\n",
      "68 loss: 0.033, training_accuracy: 0.93000\n",
      "69 loss: 0.023, training_accuracy: 0.99000\n",
      "70 loss: 0.026, training_accuracy: 0.99000\n",
      "71 loss: 0.023, training_accuracy: 0.97000\n",
      "72 loss: 0.032, training_accuracy: 0.94000\n",
      "73 loss: 0.024, training_accuracy: 0.96000\n",
      "74 loss: 0.029, training_accuracy: 0.94000\n",
      "75 loss: 0.032, training_accuracy: 0.95000\n",
      "76 loss: 0.026, training_accuracy: 0.97000\n",
      "77 loss: 0.031, training_accuracy: 0.95000\n",
      "78 loss: 0.026, training_accuracy: 0.95000\n",
      "79 loss: 0.026, training_accuracy: 0.99000\n",
      "80 loss: 0.027, training_accuracy: 0.98000\n",
      "81 loss: 0.025, training_accuracy: 0.98000\n",
      "82 loss: 0.027, training_accuracy: 0.95000\n",
      "83 loss: 0.023, training_accuracy: 0.97000\n",
      "84 loss: 0.031, training_accuracy: 0.93000\n",
      "85 loss: 0.026, training_accuracy: 0.96000\n",
      "86 loss: 0.029, training_accuracy: 0.97000\n",
      "87 loss: 0.025, training_accuracy: 1.00000\n",
      "88 loss: 0.023, training_accuracy: 0.96000\n",
      "89 loss: 0.028, training_accuracy: 0.95000\n",
      "90 loss: 0.025, training_accuracy: 0.95000\n",
      "91 loss: 0.028, training_accuracy: 0.99000\n",
      "92 loss: 0.025, training_accuracy: 0.97000\n",
      "93 loss: 0.023, training_accuracy: 0.97000\n",
      "94 loss: 0.026, training_accuracy: 0.99000\n",
      "95 loss: 0.029, training_accuracy: 0.96000\n",
      "96 loss: 0.024, training_accuracy: 0.95000\n",
      "97 loss: 0.024, training_accuracy: 0.97000\n",
      "98 loss: 0.028, training_accuracy: 0.95000\n",
      "99 loss: 0.025, training_accuracy: 0.96000\n",
      "100 loss: 0.025, training_accuracy: 0.97000\n",
      "101 loss: 0.024, training_accuracy: 0.97000\n",
      "102 loss: 0.026, training_accuracy: 0.96000\n",
      "103 loss: 0.029, training_accuracy: 0.95000\n",
      "104 loss: 0.021, training_accuracy: 0.98000\n",
      "105 loss: 0.026, training_accuracy: 0.97000\n",
      "106 loss: 0.032, training_accuracy: 0.92000\n",
      "107 loss: 0.026, training_accuracy: 0.99000\n",
      "108 loss: 0.028, training_accuracy: 0.97000\n",
      "109 loss: 0.026, training_accuracy: 0.98000\n",
      "110 loss: 0.026, training_accuracy: 0.96000\n",
      "111 loss: 0.032, training_accuracy: 0.92000\n",
      "112 loss: 0.020, training_accuracy: 0.99000\n",
      "113 loss: 0.028, training_accuracy: 0.98000\n",
      "114 loss: 0.022, training_accuracy: 1.00000\n",
      "115 loss: 0.020, training_accuracy: 0.99000\n",
      "116 loss: 0.025, training_accuracy: 0.97000\n",
      "117 loss: 0.024, training_accuracy: 0.99000\n",
      "118 loss: 0.024, training_accuracy: 0.99000\n",
      "119 loss: 0.027, training_accuracy: 0.94000\n",
      "120 loss: 0.026, training_accuracy: 0.96000\n",
      "121 loss: 0.023, training_accuracy: 0.97000\n",
      "122 loss: 0.024, training_accuracy: 0.97000\n",
      "123 loss: 0.023, training_accuracy: 0.98000\n",
      "124 loss: 0.024, training_accuracy: 0.97000\n",
      "125 loss: 0.027, training_accuracy: 0.95000\n",
      "126 loss: 0.021, training_accuracy: 0.98000\n",
      "127 loss: 0.024, training_accuracy: 0.96000\n",
      "128 loss: 0.027, training_accuracy: 0.94000\n",
      "129 loss: 0.024, training_accuracy: 0.96000\n",
      "130 loss: 0.021, training_accuracy: 0.98000\n",
      "131 loss: 0.025, training_accuracy: 0.98000\n",
      "132 loss: 0.024, training_accuracy: 0.97000\n",
      "133 loss: 0.022, training_accuracy: 0.95000\n",
      "134 loss: 0.024, training_accuracy: 0.96000\n",
      "135 loss: 0.024, training_accuracy: 0.99000\n",
      "136 loss: 0.025, training_accuracy: 0.98000\n",
      "137 loss: 0.021, training_accuracy: 0.98000\n",
      "138 loss: 0.020, training_accuracy: 0.99000\n",
      "139 loss: 0.024, training_accuracy: 0.95000\n",
      "140 loss: 0.026, training_accuracy: 0.96000\n",
      "141 loss: 0.026, training_accuracy: 0.95000\n",
      "142 loss: 0.022, training_accuracy: 0.99000\n",
      "143 loss: 0.027, training_accuracy: 0.96000\n",
      "144 loss: 0.023, training_accuracy: 0.98000\n",
      "145 loss: 0.023, training_accuracy: 0.96000\n",
      "146 loss: 0.024, training_accuracy: 0.95000\n",
      "147 loss: 0.022, training_accuracy: 0.98000\n",
      "148 loss: 0.023, training_accuracy: 0.99000\n",
      "149 loss: 0.025, training_accuracy: 0.95000\n",
      "150 loss: 0.026, training_accuracy: 0.99000\n",
      "151 loss: 0.026, training_accuracy: 0.96000\n",
      "152 loss: 0.027, training_accuracy: 0.96000\n",
      "153 loss: 0.025, training_accuracy: 0.97000\n",
      "154 loss: 0.023, training_accuracy: 0.97000\n",
      "155 loss: 0.023, training_accuracy: 0.97000\n",
      "156 loss: 0.022, training_accuracy: 0.98000\n",
      "157 loss: 0.023, training_accuracy: 0.98000\n",
      "158 loss: 0.026, training_accuracy: 0.98000\n",
      "159 loss: 0.021, training_accuracy: 0.99000\n",
      "160 loss: 0.021, training_accuracy: 0.98000\n",
      "161 loss: 0.022, training_accuracy: 0.98000\n",
      "162 loss: 0.024, training_accuracy: 0.95000\n",
      "163 loss: 0.023, training_accuracy: 0.99000\n",
      "164 loss: 0.020, training_accuracy: 1.00000\n",
      "165 loss: 0.021, training_accuracy: 0.98000\n",
      "166 loss: 0.021, training_accuracy: 0.99000\n",
      "167 loss: 0.028, training_accuracy: 1.00000\n",
      "168 loss: 0.022, training_accuracy: 0.99000\n",
      "169 loss: 0.026, training_accuracy: 0.98000\n",
      "170 loss: 0.019, training_accuracy: 0.99000\n",
      "171 loss: 0.024, training_accuracy: 0.99000\n",
      "172 loss: 0.018, training_accuracy: 0.98000\n",
      "173 loss: 0.025, training_accuracy: 0.98000\n",
      "174 loss: 0.023, training_accuracy: 0.99000\n",
      "175 loss: 0.019, training_accuracy: 0.99000\n",
      "176 loss: 0.021, training_accuracy: 0.98000\n",
      "177 loss: 0.027, training_accuracy: 0.95000\n",
      "178 loss: 0.021, training_accuracy: 0.98000\n",
      "179 loss: 0.020, training_accuracy: 0.97000\n",
      "180 loss: 0.021, training_accuracy: 0.99000\n",
      "181 loss: 0.025, training_accuracy: 0.99000\n",
      "182 loss: 0.024, training_accuracy: 0.96000\n",
      "183 loss: 0.024, training_accuracy: 0.96000\n",
      "184 loss: 0.021, training_accuracy: 1.00000\n",
      "185 loss: 0.024, training_accuracy: 0.97000\n",
      "186 loss: 0.018, training_accuracy: 1.00000\n",
      "187 loss: 0.018, training_accuracy: 1.00000\n",
      "188 loss: 0.021, training_accuracy: 0.98000\n",
      "189 loss: 0.020, training_accuracy: 0.98000\n",
      "190 loss: 0.020, training_accuracy: 0.97000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 loss: 0.022, training_accuracy: 0.98000\n",
      "192 loss: 0.018, training_accuracy: 0.98000\n",
      "193 loss: 0.022, training_accuracy: 0.98000\n",
      "194 loss: 0.019, training_accuracy: 0.98000\n",
      "195 loss: 0.024, training_accuracy: 0.94000\n",
      "196 loss: 0.022, training_accuracy: 0.97000\n",
      "197 loss: 0.020, training_accuracy: 0.98000\n",
      "198 loss: 0.025, training_accuracy: 0.99000\n",
      "199 loss: 0.020, training_accuracy: 1.00000\n",
      "200 loss: 0.023, training_accuracy: 0.98000\n",
      "201 loss: 0.024, training_accuracy: 1.00000\n",
      "202 loss: 0.020, training_accuracy: 0.99000\n",
      "203 loss: 0.016, training_accuracy: 1.00000\n",
      "204 loss: 0.021, training_accuracy: 0.99000\n",
      "205 loss: 0.022, training_accuracy: 0.98000\n",
      "206 loss: 0.019, training_accuracy: 0.99000\n",
      "207 loss: 0.019, training_accuracy: 0.99000\n",
      "208 loss: 0.019, training_accuracy: 0.98000\n",
      "209 loss: 0.019, training_accuracy: 0.98000\n",
      "210 loss: 0.018, training_accuracy: 1.00000\n",
      "211 loss: 0.021, training_accuracy: 0.96000\n",
      "212 loss: 0.021, training_accuracy: 0.97000\n",
      "213 loss: 0.021, training_accuracy: 0.98000\n",
      "214 loss: 0.020, training_accuracy: 0.99000\n",
      "215 loss: 0.019, training_accuracy: 1.00000\n",
      "216 loss: 0.018, training_accuracy: 0.99000\n",
      "217 loss: 0.022, training_accuracy: 0.98000\n",
      "218 loss: 0.018, training_accuracy: 0.97000\n",
      "219 loss: 0.023, training_accuracy: 0.99000\n",
      "220 loss: 0.017, training_accuracy: 0.99000\n",
      "221 loss: 0.023, training_accuracy: 0.98000\n",
      "222 loss: 0.018, training_accuracy: 0.99000\n",
      "223 loss: 0.019, training_accuracy: 0.98000\n",
      "224 loss: 0.021, training_accuracy: 0.98000\n",
      "225 loss: 0.021, training_accuracy: 0.99000\n",
      "226 loss: 0.021, training_accuracy: 1.00000\n",
      "227 loss: 0.017, training_accuracy: 0.98000\n",
      "228 loss: 0.021, training_accuracy: 0.97000\n",
      "229 loss: 0.020, training_accuracy: 0.99000\n",
      "230 loss: 0.021, training_accuracy: 0.99000\n",
      "231 loss: 0.020, training_accuracy: 1.00000\n",
      "232 loss: 0.018, training_accuracy: 1.00000\n",
      "233 loss: 0.021, training_accuracy: 0.99000\n",
      "234 loss: 0.022, training_accuracy: 0.98000\n",
      "235 loss: 0.021, training_accuracy: 0.99000\n",
      "236 loss: 0.021, training_accuracy: 0.98000\n",
      "237 loss: 0.023, training_accuracy: 0.99000\n",
      "238 loss: 0.015, training_accuracy: 1.00000\n",
      "239 loss: 0.019, training_accuracy: 1.00000\n",
      "240 loss: 0.018, training_accuracy: 0.97000\n",
      "241 loss: 0.022, training_accuracy: 0.97000\n",
      "242 loss: 0.018, training_accuracy: 0.99000\n",
      "243 loss: 0.022, training_accuracy: 0.98000\n",
      "244 loss: 0.024, training_accuracy: 0.97000\n",
      "245 loss: 0.021, training_accuracy: 0.98000\n",
      "246 loss: 0.017, training_accuracy: 1.00000\n",
      "247 loss: 0.020, training_accuracy: 1.00000\n",
      "248 loss: 0.025, training_accuracy: 0.96000\n",
      "249 loss: 0.022, training_accuracy: 0.98000\n",
      "250 loss: 0.021, training_accuracy: 0.98000\n",
      "251 loss: 0.019, training_accuracy: 1.00000\n",
      "252 loss: 0.018, training_accuracy: 1.00000\n",
      "253 loss: 0.017, training_accuracy: 0.99000\n",
      "254 loss: 0.021, training_accuracy: 0.98000\n",
      "255 loss: 0.018, training_accuracy: 1.00000\n",
      "256 loss: 0.023, training_accuracy: 0.99000\n",
      "257 loss: 0.018, training_accuracy: 0.98000\n",
      "258 loss: 0.022, training_accuracy: 0.97000\n",
      "259 loss: 0.015, training_accuracy: 1.00000\n",
      "260 loss: 0.022, training_accuracy: 0.98000\n",
      "261 loss: 0.019, training_accuracy: 1.00000\n",
      "262 loss: 0.016, training_accuracy: 1.00000\n",
      "263 loss: 0.017, training_accuracy: 0.98000\n",
      "264 loss: 0.020, training_accuracy: 0.99000\n",
      "265 loss: 0.017, training_accuracy: 1.00000\n",
      "266 loss: 0.019, training_accuracy: 0.97000\n",
      "267 loss: 0.019, training_accuracy: 0.98000\n",
      "268 loss: 0.020, training_accuracy: 0.97000\n",
      "269 loss: 0.020, training_accuracy: 0.98000\n",
      "270 loss: 0.015, training_accuracy: 0.99000\n",
      "271 loss: 0.019, training_accuracy: 0.99000\n",
      "272 loss: 0.016, training_accuracy: 1.00000\n",
      "273 loss: 0.024, training_accuracy: 0.98000\n",
      "274 loss: 0.019, training_accuracy: 0.99000\n",
      "275 loss: 0.018, training_accuracy: 0.98000\n",
      "276 loss: 0.018, training_accuracy: 0.98000\n",
      "277 loss: 0.018, training_accuracy: 1.00000\n",
      "278 loss: 0.019, training_accuracy: 0.99000\n",
      "279 loss: 0.016, training_accuracy: 1.00000\n",
      "280 loss: 0.017, training_accuracy: 0.99000\n",
      "281 loss: 0.021, training_accuracy: 0.98000\n",
      "282 loss: 0.020, training_accuracy: 0.98000\n",
      "283 loss: 0.019, training_accuracy: 0.98000\n",
      "284 loss: 0.016, training_accuracy: 1.00000\n",
      "285 loss: 0.019, training_accuracy: 1.00000\n",
      "286 loss: 0.017, training_accuracy: 0.99000\n",
      "287 loss: 0.017, training_accuracy: 0.99000\n",
      "288 loss: 0.021, training_accuracy: 0.99000\n",
      "289 loss: 0.019, training_accuracy: 0.98000\n",
      "290 loss: 0.019, training_accuracy: 0.99000\n",
      "291 loss: 0.016, training_accuracy: 0.98000\n",
      "292 loss: 0.021, training_accuracy: 0.99000\n",
      "293 loss: 0.016, training_accuracy: 1.00000\n",
      "294 loss: 0.017, training_accuracy: 0.99000\n",
      "295 loss: 0.015, training_accuracy: 1.00000\n",
      "296 loss: 0.019, training_accuracy: 1.00000\n",
      "297 loss: 0.017, training_accuracy: 0.98000\n",
      "298 loss: 0.018, training_accuracy: 0.98000\n",
      "299 loss: 0.019, training_accuracy: 1.00000\n",
      "300 loss: 0.020, training_accuracy: 0.98000\n",
      "301 loss: 0.015, training_accuracy: 0.99000\n",
      "302 loss: 0.018, training_accuracy: 0.99000\n",
      "303 loss: 0.018, training_accuracy: 0.96000\n",
      "304 loss: 0.020, training_accuracy: 0.96000\n",
      "305 loss: 0.019, training_accuracy: 0.97000\n",
      "306 loss: 0.016, training_accuracy: 0.99000\n",
      "307 loss: 0.020, training_accuracy: 1.00000\n",
      "308 loss: 0.018, training_accuracy: 0.99000\n",
      "309 loss: 0.016, training_accuracy: 1.00000\n",
      "310 loss: 0.015, training_accuracy: 0.99000\n",
      "311 loss: 0.016, training_accuracy: 0.98000\n",
      "312 loss: 0.021, training_accuracy: 1.00000\n",
      "313 loss: 0.016, training_accuracy: 1.00000\n",
      "314 loss: 0.018, training_accuracy: 1.00000\n",
      "315 loss: 0.019, training_accuracy: 0.98000\n",
      "316 loss: 0.021, training_accuracy: 0.99000\n",
      "317 loss: 0.015, training_accuracy: 0.99000\n",
      "318 loss: 0.018, training_accuracy: 1.00000\n",
      "319 loss: 0.016, training_accuracy: 1.00000\n",
      "320 loss: 0.016, training_accuracy: 0.97000\n",
      "321 loss: 0.014, training_accuracy: 0.99000\n",
      "322 loss: 0.017, training_accuracy: 1.00000\n",
      "323 loss: 0.016, training_accuracy: 1.00000\n",
      "324 loss: 0.016, training_accuracy: 0.99000\n",
      "325 loss: 0.017, training_accuracy: 0.98000\n",
      "326 loss: 0.018, training_accuracy: 0.98000\n",
      "327 loss: 0.019, training_accuracy: 0.98000\n",
      "328 loss: 0.014, training_accuracy: 0.99000\n",
      "329 loss: 0.016, training_accuracy: 1.00000\n",
      "330 loss: 0.016, training_accuracy: 0.99000\n",
      "331 loss: 0.012, training_accuracy: 1.00000\n",
      "332 loss: 0.014, training_accuracy: 1.00000\n",
      "333 loss: 0.020, training_accuracy: 0.97000\n",
      "334 loss: 0.018, training_accuracy: 0.99000\n",
      "335 loss: 0.018, training_accuracy: 0.99000\n",
      "336 loss: 0.017, training_accuracy: 1.00000\n",
      "337 loss: 0.017, training_accuracy: 0.99000\n",
      "338 loss: 0.017, training_accuracy: 1.00000\n",
      "339 loss: 0.018, training_accuracy: 0.98000\n",
      "340 loss: 0.015, training_accuracy: 0.99000\n",
      "341 loss: 0.023, training_accuracy: 0.98000\n",
      "342 loss: 0.016, training_accuracy: 1.00000\n",
      "343 loss: 0.017, training_accuracy: 0.96000\n",
      "344 loss: 0.018, training_accuracy: 0.99000\n",
      "345 loss: 0.016, training_accuracy: 1.00000\n",
      "346 loss: 0.018, training_accuracy: 1.00000\n",
      "347 loss: 0.018, training_accuracy: 0.98000\n",
      "348 loss: 0.018, training_accuracy: 1.00000\n",
      "349 loss: 0.016, training_accuracy: 1.00000\n",
      "350 loss: 0.017, training_accuracy: 1.00000\n",
      "351 loss: 0.015, training_accuracy: 1.00000\n",
      "352 loss: 0.018, training_accuracy: 0.99000\n",
      "353 loss: 0.016, training_accuracy: 0.97000\n",
      "354 loss: 0.016, training_accuracy: 0.99000\n",
      "355 loss: 0.014, training_accuracy: 1.00000\n",
      "356 loss: 0.017, training_accuracy: 1.00000\n",
      "357 loss: 0.014, training_accuracy: 1.00000\n",
      "358 loss: 0.016, training_accuracy: 0.98000\n",
      "359 loss: 0.015, training_accuracy: 0.98000\n",
      "360 loss: 0.015, training_accuracy: 1.00000\n",
      "361 loss: 0.015, training_accuracy: 1.00000\n",
      "362 loss: 0.018, training_accuracy: 1.00000\n",
      "363 loss: 0.019, training_accuracy: 1.00000\n",
      "364 loss: 0.017, training_accuracy: 1.00000\n",
      "365 loss: 0.018, training_accuracy: 0.99000\n",
      "366 loss: 0.015, training_accuracy: 1.00000\n",
      "367 loss: 0.014, training_accuracy: 0.98000\n",
      "368 loss: 0.016, training_accuracy: 1.00000\n",
      "369 loss: 0.015, training_accuracy: 1.00000\n",
      "370 loss: 0.016, training_accuracy: 1.00000\n",
      "371 loss: 0.016, training_accuracy: 1.00000\n",
      "372 loss: 0.016, training_accuracy: 1.00000\n",
      "373 loss: 0.016, training_accuracy: 0.99000\n",
      "374 loss: 0.013, training_accuracy: 0.99000\n",
      "375 loss: 0.016, training_accuracy: 1.00000\n",
      "376 loss: 0.018, training_accuracy: 1.00000\n",
      "377 loss: 0.015, training_accuracy: 1.00000\n",
      "378 loss: 0.017, training_accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379 loss: 0.014, training_accuracy: 1.00000\n",
      "380 loss: 0.015, training_accuracy: 1.00000\n",
      "381 loss: 0.014, training_accuracy: 0.99000\n",
      "382 loss: 0.016, training_accuracy: 0.99000\n",
      "383 loss: 0.011, training_accuracy: 1.00000\n",
      "384 loss: 0.013, training_accuracy: 1.00000\n",
      "385 loss: 0.017, training_accuracy: 1.00000\n",
      "386 loss: 0.014, training_accuracy: 1.00000\n",
      "387 loss: 0.019, training_accuracy: 0.97000\n",
      "388 loss: 0.017, training_accuracy: 0.99000\n",
      "389 loss: 0.013, training_accuracy: 0.98000\n",
      "390 loss: 0.014, training_accuracy: 0.98000\n",
      "391 loss: 0.017, training_accuracy: 1.00000\n",
      "392 loss: 0.017, training_accuracy: 0.99000\n",
      "393 loss: 0.021, training_accuracy: 0.97000\n",
      "394 loss: 0.016, training_accuracy: 1.00000\n",
      "395 loss: 0.016, training_accuracy: 1.00000\n",
      "396 loss: 0.014, training_accuracy: 0.99000\n",
      "397 loss: 0.014, training_accuracy: 1.00000\n",
      "398 loss: 0.016, training_accuracy: 1.00000\n",
      "399 loss: 0.015, training_accuracy: 1.00000\n",
      "400 loss: 0.014, training_accuracy: 1.00000\n",
      "401 loss: 0.016, training_accuracy: 1.00000\n",
      "402 loss: 0.013, training_accuracy: 0.99000\n",
      "403 loss: 0.012, training_accuracy: 1.00000\n",
      "404 loss: 0.015, training_accuracy: 1.00000\n",
      "405 loss: 0.017, training_accuracy: 0.99000\n",
      "406 loss: 0.019, training_accuracy: 0.99000\n",
      "407 loss: 0.016, training_accuracy: 0.99000\n",
      "408 loss: 0.017, training_accuracy: 0.99000\n",
      "409 loss: 0.018, training_accuracy: 1.00000\n",
      "410 loss: 0.014, training_accuracy: 1.00000\n",
      "411 loss: 0.017, training_accuracy: 0.98000\n",
      "412 loss: 0.015, training_accuracy: 0.99000\n",
      "413 loss: 0.017, training_accuracy: 0.98000\n",
      "414 loss: 0.015, training_accuracy: 0.98000\n",
      "415 loss: 0.016, training_accuracy: 1.00000\n",
      "416 loss: 0.015, training_accuracy: 0.99000\n",
      "417 loss: 0.012, training_accuracy: 1.00000\n",
      "418 loss: 0.017, training_accuracy: 1.00000\n",
      "419 loss: 0.017, training_accuracy: 1.00000\n",
      "420 loss: 0.013, training_accuracy: 1.00000\n",
      "421 loss: 0.012, training_accuracy: 1.00000\n",
      "422 loss: 0.018, training_accuracy: 1.00000\n",
      "423 loss: 0.016, training_accuracy: 1.00000\n",
      "424 loss: 0.015, training_accuracy: 1.00000\n",
      "425 loss: 0.015, training_accuracy: 1.00000\n",
      "426 loss: 0.016, training_accuracy: 0.99000\n",
      "427 loss: 0.013, training_accuracy: 0.99000\n",
      "428 loss: 0.012, training_accuracy: 1.00000\n",
      "429 loss: 0.018, training_accuracy: 0.98000\n",
      "430 loss: 0.015, training_accuracy: 0.98000\n",
      "431 loss: 0.012, training_accuracy: 1.00000\n",
      "432 loss: 0.014, training_accuracy: 1.00000\n",
      "433 loss: 0.016, training_accuracy: 1.00000\n",
      "434 loss: 0.014, training_accuracy: 1.00000\n",
      "435 loss: 0.013, training_accuracy: 1.00000\n",
      "436 loss: 0.016, training_accuracy: 1.00000\n",
      "437 loss: 0.013, training_accuracy: 1.00000\n",
      "438 loss: 0.015, training_accuracy: 1.00000\n",
      "439 loss: 0.015, training_accuracy: 1.00000\n",
      "440 loss: 0.015, training_accuracy: 0.99000\n",
      "441 loss: 0.015, training_accuracy: 1.00000\n",
      "442 loss: 0.013, training_accuracy: 1.00000\n",
      "443 loss: 0.012, training_accuracy: 0.99000\n",
      "444 loss: 0.016, training_accuracy: 0.97000\n",
      "445 loss: 0.014, training_accuracy: 1.00000\n",
      "446 loss: 0.013, training_accuracy: 1.00000\n",
      "447 loss: 0.014, training_accuracy: 1.00000\n",
      "448 loss: 0.017, training_accuracy: 1.00000\n",
      "449 loss: 0.013, training_accuracy: 1.00000\n",
      "450 loss: 0.013, training_accuracy: 1.00000\n",
      "451 loss: 0.015, training_accuracy: 1.00000\n",
      "452 loss: 0.016, training_accuracy: 1.00000\n",
      "453 loss: 0.015, training_accuracy: 1.00000\n",
      "454 loss: 0.015, training_accuracy: 0.99000\n",
      "455 loss: 0.012, training_accuracy: 1.00000\n",
      "456 loss: 0.017, training_accuracy: 0.98000\n",
      "457 loss: 0.014, training_accuracy: 1.00000\n",
      "458 loss: 0.013, training_accuracy: 1.00000\n",
      "459 loss: 0.015, training_accuracy: 1.00000\n",
      "460 loss: 0.014, training_accuracy: 1.00000\n",
      "461 loss: 0.017, training_accuracy: 1.00000\n",
      "462 loss: 0.015, training_accuracy: 1.00000\n",
      "463 loss: 0.011, training_accuracy: 1.00000\n",
      "464 loss: 0.016, training_accuracy: 0.98000\n",
      "465 loss: 0.013, training_accuracy: 0.98000\n",
      "466 loss: 0.016, training_accuracy: 1.00000\n",
      "467 loss: 0.016, training_accuracy: 1.00000\n",
      "468 loss: 0.013, training_accuracy: 1.00000\n",
      "469 loss: 0.013, training_accuracy: 1.00000\n",
      "470 loss: 0.015, training_accuracy: 1.00000\n",
      "471 loss: 0.017, training_accuracy: 1.00000\n",
      "472 loss: 0.011, training_accuracy: 1.00000\n",
      "473 loss: 0.012, training_accuracy: 1.00000\n",
      "474 loss: 0.015, training_accuracy: 1.00000\n",
      "475 loss: 0.014, training_accuracy: 0.99000\n",
      "476 loss: 0.014, training_accuracy: 1.00000\n",
      "477 loss: 0.015, training_accuracy: 1.00000\n",
      "478 loss: 0.013, training_accuracy: 1.00000\n",
      "479 loss: 0.014, training_accuracy: 1.00000\n",
      "480 loss: 0.017, training_accuracy: 1.00000\n",
      "481 loss: 0.012, training_accuracy: 1.00000\n",
      "482 loss: 0.012, training_accuracy: 1.00000\n",
      "483 loss: 0.015, training_accuracy: 0.99000\n",
      "484 loss: 0.012, training_accuracy: 1.00000\n",
      "485 loss: 0.017, training_accuracy: 1.00000\n",
      "486 loss: 0.012, training_accuracy: 1.00000\n",
      "487 loss: 0.013, training_accuracy: 1.00000\n",
      "488 loss: 0.012, training_accuracy: 0.99000\n",
      "489 loss: 0.014, training_accuracy: 1.00000\n",
      "490 loss: 0.015, training_accuracy: 1.00000\n",
      "491 loss: 0.013, training_accuracy: 1.00000\n",
      "492 loss: 0.011, training_accuracy: 1.00000\n",
      "493 loss: 0.014, training_accuracy: 1.00000\n",
      "494 loss: 0.013, training_accuracy: 1.00000\n",
      "495 loss: 0.012, training_accuracy: 1.00000\n",
      "496 loss: 0.014, training_accuracy: 0.99000\n",
      "497 loss: 0.015, training_accuracy: 1.00000\n",
      "498 loss: 0.014, training_accuracy: 0.99000\n",
      "499 loss: 0.014, training_accuracy: 1.00000\n",
      "500 loss: 0.013, training_accuracy: 1.00000\n",
      "501 loss: 0.013, training_accuracy: 1.00000\n",
      "502 loss: 0.012, training_accuracy: 1.00000\n",
      "503 loss: 0.013, training_accuracy: 1.00000\n",
      "504 loss: 0.013, training_accuracy: 1.00000\n",
      "505 loss: 0.010, training_accuracy: 1.00000\n",
      "506 loss: 0.013, training_accuracy: 1.00000\n",
      "507 loss: 0.016, training_accuracy: 1.00000\n",
      "508 loss: 0.015, training_accuracy: 1.00000\n",
      "509 loss: 0.013, training_accuracy: 1.00000\n",
      "510 loss: 0.014, training_accuracy: 1.00000\n",
      "511 loss: 0.011, training_accuracy: 1.00000\n",
      "512 loss: 0.015, training_accuracy: 1.00000\n",
      "513 loss: 0.014, training_accuracy: 0.99000\n",
      "514 loss: 0.014, training_accuracy: 1.00000\n",
      "515 loss: 0.013, training_accuracy: 1.00000\n",
      "516 loss: 0.011, training_accuracy: 1.00000\n",
      "517 loss: 0.013, training_accuracy: 0.98000\n",
      "518 loss: 0.010, training_accuracy: 1.00000\n",
      "519 loss: 0.014, training_accuracy: 0.97000\n",
      "520 loss: 0.014, training_accuracy: 1.00000\n",
      "521 loss: 0.010, training_accuracy: 1.00000\n",
      "522 loss: 0.011, training_accuracy: 1.00000\n",
      "523 loss: 0.012, training_accuracy: 1.00000\n",
      "524 loss: 0.011, training_accuracy: 0.99000\n",
      "525 loss: 0.014, training_accuracy: 1.00000\n",
      "526 loss: 0.012, training_accuracy: 1.00000\n",
      "527 loss: 0.013, training_accuracy: 1.00000\n",
      "528 loss: 0.012, training_accuracy: 1.00000\n",
      "529 loss: 0.010, training_accuracy: 1.00000\n",
      "530 loss: 0.014, training_accuracy: 1.00000\n",
      "531 loss: 0.014, training_accuracy: 1.00000\n",
      "532 loss: 0.013, training_accuracy: 1.00000\n",
      "533 loss: 0.011, training_accuracy: 0.99000\n",
      "534 loss: 0.011, training_accuracy: 1.00000\n",
      "535 loss: 0.012, training_accuracy: 1.00000\n",
      "536 loss: 0.012, training_accuracy: 1.00000\n",
      "537 loss: 0.014, training_accuracy: 1.00000\n",
      "538 loss: 0.014, training_accuracy: 1.00000\n",
      "539 loss: 0.013, training_accuracy: 1.00000\n",
      "540 loss: 0.012, training_accuracy: 1.00000\n",
      "541 loss: 0.012, training_accuracy: 1.00000\n",
      "542 loss: 0.011, training_accuracy: 1.00000\n",
      "543 loss: 0.013, training_accuracy: 1.00000\n",
      "544 loss: 0.012, training_accuracy: 1.00000\n",
      "545 loss: 0.014, training_accuracy: 1.00000\n",
      "546 loss: 0.013, training_accuracy: 1.00000\n",
      "547 loss: 0.010, training_accuracy: 1.00000\n",
      "548 loss: 0.015, training_accuracy: 0.99000\n",
      "549 loss: 0.014, training_accuracy: 1.00000\n",
      "550 loss: 0.012, training_accuracy: 1.00000\n",
      "551 loss: 0.012, training_accuracy: 1.00000\n",
      "552 loss: 0.011, training_accuracy: 1.00000\n",
      "553 loss: 0.014, training_accuracy: 1.00000\n",
      "554 loss: 0.011, training_accuracy: 1.00000\n",
      "555 loss: 0.014, training_accuracy: 1.00000\n",
      "556 loss: 0.015, training_accuracy: 1.00000\n",
      "557 loss: 0.010, training_accuracy: 1.00000\n",
      "558 loss: 0.012, training_accuracy: 1.00000\n",
      "559 loss: 0.016, training_accuracy: 1.00000\n",
      "560 loss: 0.013, training_accuracy: 1.00000\n",
      "561 loss: 0.013, training_accuracy: 1.00000\n",
      "562 loss: 0.013, training_accuracy: 1.00000\n",
      "563 loss: 0.016, training_accuracy: 1.00000\n",
      "564 loss: 0.012, training_accuracy: 1.00000\n",
      "565 loss: 0.009, training_accuracy: 1.00000\n",
      "566 loss: 0.011, training_accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 loss: 0.013, training_accuracy: 1.00000\n",
      "568 loss: 0.013, training_accuracy: 1.00000\n",
      "569 loss: 0.014, training_accuracy: 1.00000\n",
      "570 loss: 0.013, training_accuracy: 1.00000\n",
      "571 loss: 0.012, training_accuracy: 1.00000\n",
      "572 loss: 0.014, training_accuracy: 1.00000\n",
      "573 loss: 0.012, training_accuracy: 1.00000\n",
      "574 loss: 0.014, training_accuracy: 1.00000\n",
      "575 loss: 0.011, training_accuracy: 1.00000\n",
      "576 loss: 0.011, training_accuracy: 1.00000\n",
      "577 loss: 0.012, training_accuracy: 1.00000\n",
      "578 loss: 0.010, training_accuracy: 1.00000\n",
      "579 loss: 0.012, training_accuracy: 1.00000\n",
      "580 loss: 0.012, training_accuracy: 1.00000\n",
      "581 loss: 0.013, training_accuracy: 1.00000\n",
      "582 loss: 0.013, training_accuracy: 1.00000\n",
      "583 loss: 0.013, training_accuracy: 1.00000\n",
      "584 loss: 0.014, training_accuracy: 1.00000\n",
      "585 loss: 0.011, training_accuracy: 1.00000\n",
      "586 loss: 0.010, training_accuracy: 1.00000\n",
      "587 loss: 0.014, training_accuracy: 1.00000\n",
      "588 loss: 0.012, training_accuracy: 1.00000\n",
      "589 loss: 0.013, training_accuracy: 1.00000\n",
      "590 loss: 0.012, training_accuracy: 1.00000\n",
      "591 loss: 0.009, training_accuracy: 1.00000\n",
      "592 loss: 0.011, training_accuracy: 1.00000\n",
      "593 loss: 0.011, training_accuracy: 1.00000\n",
      "594 loss: 0.016, training_accuracy: 1.00000\n",
      "595 loss: 0.015, training_accuracy: 1.00000\n",
      "596 loss: 0.011, training_accuracy: 1.00000\n",
      "597 loss: 0.012, training_accuracy: 1.00000\n",
      "598 loss: 0.013, training_accuracy: 1.00000\n",
      "599 loss: 0.011, training_accuracy: 1.00000\n",
      "600 loss: 0.014, training_accuracy: 1.00000\n",
      "601 loss: 0.012, training_accuracy: 1.00000\n",
      "602 loss: 0.011, training_accuracy: 1.00000\n",
      "603 loss: 0.012, training_accuracy: 1.00000\n",
      "604 loss: 0.011, training_accuracy: 1.00000\n",
      "605 loss: 0.013, training_accuracy: 1.00000\n",
      "606 loss: 0.010, training_accuracy: 1.00000\n",
      "607 loss: 0.011, training_accuracy: 1.00000\n",
      "608 loss: 0.010, training_accuracy: 1.00000\n",
      "609 loss: 0.012, training_accuracy: 1.00000\n",
      "610 loss: 0.011, training_accuracy: 1.00000\n",
      "611 loss: 0.011, training_accuracy: 1.00000\n",
      "612 loss: 0.010, training_accuracy: 1.00000\n",
      "613 loss: 0.012, training_accuracy: 1.00000\n",
      "614 loss: 0.009, training_accuracy: 1.00000\n",
      "615 loss: 0.013, training_accuracy: 1.00000\n",
      "616 loss: 0.010, training_accuracy: 1.00000\n",
      "617 loss: 0.009, training_accuracy: 1.00000\n",
      "618 loss: 0.011, training_accuracy: 1.00000\n",
      "619 loss: 0.011, training_accuracy: 1.00000\n",
      "620 loss: 0.013, training_accuracy: 1.00000\n",
      "621 loss: 0.013, training_accuracy: 1.00000\n",
      "622 loss: 0.011, training_accuracy: 1.00000\n",
      "623 loss: 0.012, training_accuracy: 1.00000\n",
      "624 loss: 0.012, training_accuracy: 1.00000\n",
      "625 loss: 0.011, training_accuracy: 1.00000\n",
      "626 loss: 0.012, training_accuracy: 1.00000\n",
      "627 loss: 0.009, training_accuracy: 1.00000\n",
      "628 loss: 0.010, training_accuracy: 1.00000\n",
      "629 loss: 0.011, training_accuracy: 1.00000\n",
      "630 loss: 0.010, training_accuracy: 1.00000\n",
      "631 loss: 0.013, training_accuracy: 1.00000\n",
      "632 loss: 0.012, training_accuracy: 1.00000\n",
      "633 loss: 0.011, training_accuracy: 1.00000\n",
      "634 loss: 0.011, training_accuracy: 1.00000\n",
      "635 loss: 0.013, training_accuracy: 1.00000\n",
      "636 loss: 0.013, training_accuracy: 1.00000\n",
      "637 loss: 0.010, training_accuracy: 1.00000\n",
      "638 loss: 0.010, training_accuracy: 1.00000\n",
      "639 loss: 0.012, training_accuracy: 1.00000\n",
      "640 loss: 0.010, training_accuracy: 1.00000\n",
      "641 loss: 0.010, training_accuracy: 1.00000\n",
      "642 loss: 0.011, training_accuracy: 1.00000\n",
      "643 loss: 0.011, training_accuracy: 1.00000\n",
      "644 loss: 0.013, training_accuracy: 1.00000\n",
      "645 loss: 0.011, training_accuracy: 1.00000\n",
      "646 loss: 0.014, training_accuracy: 1.00000\n",
      "647 loss: 0.014, training_accuracy: 1.00000\n",
      "648 loss: 0.010, training_accuracy: 1.00000\n",
      "649 loss: 0.011, training_accuracy: 1.00000\n",
      "650 loss: 0.009, training_accuracy: 1.00000\n",
      "651 loss: 0.012, training_accuracy: 1.00000\n",
      "652 loss: 0.011, training_accuracy: 1.00000\n",
      "653 loss: 0.014, training_accuracy: 1.00000\n",
      "654 loss: 0.010, training_accuracy: 1.00000\n",
      "655 loss: 0.011, training_accuracy: 1.00000\n",
      "656 loss: 0.009, training_accuracy: 1.00000\n",
      "657 loss: 0.010, training_accuracy: 1.00000\n",
      "658 loss: 0.013, training_accuracy: 1.00000\n",
      "659 loss: 0.010, training_accuracy: 1.00000\n",
      "660 loss: 0.011, training_accuracy: 1.00000\n",
      "661 loss: 0.011, training_accuracy: 1.00000\n",
      "662 loss: 0.010, training_accuracy: 1.00000\n",
      "663 loss: 0.012, training_accuracy: 1.00000\n",
      "664 loss: 0.010, training_accuracy: 1.00000\n",
      "665 loss: 0.013, training_accuracy: 1.00000\n",
      "666 loss: 0.009, training_accuracy: 1.00000\n",
      "667 loss: 0.011, training_accuracy: 1.00000\n",
      "668 loss: 0.009, training_accuracy: 1.00000\n",
      "669 loss: 0.012, training_accuracy: 1.00000\n",
      "670 loss: 0.012, training_accuracy: 1.00000\n",
      "671 loss: 0.011, training_accuracy: 1.00000\n",
      "672 loss: 0.010, training_accuracy: 1.00000\n",
      "673 loss: 0.011, training_accuracy: 1.00000\n",
      "674 loss: 0.011, training_accuracy: 1.00000\n",
      "675 loss: 0.012, training_accuracy: 1.00000\n",
      "676 loss: 0.009, training_accuracy: 1.00000\n",
      "677 loss: 0.012, training_accuracy: 1.00000\n",
      "678 loss: 0.011, training_accuracy: 1.00000\n",
      "679 loss: 0.010, training_accuracy: 1.00000\n",
      "680 loss: 0.010, training_accuracy: 1.00000\n",
      "681 loss: 0.010, training_accuracy: 1.00000\n",
      "682 loss: 0.010, training_accuracy: 1.00000\n",
      "683 loss: 0.012, training_accuracy: 1.00000\n",
      "684 loss: 0.010, training_accuracy: 1.00000\n",
      "685 loss: 0.012, training_accuracy: 1.00000\n",
      "686 loss: 0.009, training_accuracy: 1.00000\n",
      "687 loss: 0.013, training_accuracy: 1.00000\n",
      "688 loss: 0.011, training_accuracy: 1.00000\n",
      "689 loss: 0.009, training_accuracy: 1.00000\n",
      "690 loss: 0.009, training_accuracy: 1.00000\n",
      "691 loss: 0.012, training_accuracy: 1.00000\n",
      "692 loss: 0.013, training_accuracy: 1.00000\n",
      "693 loss: 0.012, training_accuracy: 1.00000\n",
      "694 loss: 0.011, training_accuracy: 1.00000\n",
      "695 loss: 0.010, training_accuracy: 1.00000\n",
      "696 loss: 0.010, training_accuracy: 1.00000\n",
      "697 loss: 0.011, training_accuracy: 1.00000\n",
      "698 loss: 0.009, training_accuracy: 1.00000\n",
      "699 loss: 0.011, training_accuracy: 1.00000\n",
      "700 loss: 0.010, training_accuracy: 1.00000\n",
      "701 loss: 0.011, training_accuracy: 1.00000\n",
      "702 loss: 0.010, training_accuracy: 1.00000\n",
      "703 loss: 0.009, training_accuracy: 1.00000\n",
      "704 loss: 0.010, training_accuracy: 1.00000\n",
      "705 loss: 0.009, training_accuracy: 1.00000\n",
      "706 loss: 0.010, training_accuracy: 1.00000\n",
      "707 loss: 0.010, training_accuracy: 1.00000\n",
      "708 loss: 0.009, training_accuracy: 1.00000\n",
      "709 loss: 0.010, training_accuracy: 1.00000\n",
      "710 loss: 0.011, training_accuracy: 1.00000\n",
      "711 loss: 0.012, training_accuracy: 1.00000\n",
      "712 loss: 0.011, training_accuracy: 1.00000\n",
      "713 loss: 0.010, training_accuracy: 1.00000\n",
      "714 loss: 0.009, training_accuracy: 1.00000\n",
      "715 loss: 0.009, training_accuracy: 1.00000\n",
      "716 loss: 0.009, training_accuracy: 1.00000\n",
      "717 loss: 0.010, training_accuracy: 1.00000\n",
      "718 loss: 0.010, training_accuracy: 1.00000\n",
      "719 loss: 0.011, training_accuracy: 1.00000\n",
      "720 loss: 0.010, training_accuracy: 1.00000\n",
      "721 loss: 0.009, training_accuracy: 1.00000\n",
      "722 loss: 0.010, training_accuracy: 1.00000\n",
      "723 loss: 0.012, training_accuracy: 1.00000\n",
      "724 loss: 0.012, training_accuracy: 1.00000\n",
      "725 loss: 0.012, training_accuracy: 1.00000\n",
      "726 loss: 0.011, training_accuracy: 1.00000\n",
      "727 loss: 0.012, training_accuracy: 1.00000\n",
      "728 loss: 0.010, training_accuracy: 1.00000\n",
      "729 loss: 0.010, training_accuracy: 1.00000\n",
      "730 loss: 0.009, training_accuracy: 1.00000\n",
      "731 loss: 0.009, training_accuracy: 1.00000\n",
      "732 loss: 0.012, training_accuracy: 1.00000\n",
      "733 loss: 0.011, training_accuracy: 1.00000\n",
      "734 loss: 0.012, training_accuracy: 1.00000\n",
      "735 loss: 0.011, training_accuracy: 1.00000\n",
      "736 loss: 0.009, training_accuracy: 1.00000\n",
      "737 loss: 0.010, training_accuracy: 1.00000\n",
      "738 loss: 0.011, training_accuracy: 1.00000\n",
      "739 loss: 0.009, training_accuracy: 1.00000\n",
      "740 loss: 0.009, training_accuracy: 1.00000\n",
      "741 loss: 0.011, training_accuracy: 1.00000\n",
      "742 loss: 0.009, training_accuracy: 1.00000\n",
      "743 loss: 0.011, training_accuracy: 1.00000\n",
      "744 loss: 0.010, training_accuracy: 1.00000\n",
      "745 loss: 0.009, training_accuracy: 1.00000\n",
      "746 loss: 0.011, training_accuracy: 1.00000\n",
      "747 loss: 0.012, training_accuracy: 1.00000\n",
      "748 loss: 0.008, training_accuracy: 1.00000\n",
      "749 loss: 0.009, training_accuracy: 1.00000\n",
      "750 loss: 0.010, training_accuracy: 1.00000\n",
      "751 loss: 0.009, training_accuracy: 1.00000\n",
      "752 loss: 0.011, training_accuracy: 1.00000\n",
      "753 loss: 0.009, training_accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754 loss: 0.010, training_accuracy: 1.00000\n",
      "755 loss: 0.010, training_accuracy: 1.00000\n",
      "756 loss: 0.012, training_accuracy: 1.00000\n",
      "757 loss: 0.008, training_accuracy: 1.00000\n",
      "758 loss: 0.012, training_accuracy: 1.00000\n",
      "759 loss: 0.010, training_accuracy: 1.00000\n",
      "760 loss: 0.010, training_accuracy: 1.00000\n",
      "761 loss: 0.010, training_accuracy: 1.00000\n",
      "762 loss: 0.011, training_accuracy: 1.00000\n",
      "763 loss: 0.010, training_accuracy: 1.00000\n",
      "764 loss: 0.009, training_accuracy: 1.00000\n",
      "765 loss: 0.009, training_accuracy: 1.00000\n",
      "766 loss: 0.011, training_accuracy: 1.00000\n",
      "767 loss: 0.009, training_accuracy: 1.00000\n",
      "768 loss: 0.009, training_accuracy: 1.00000\n",
      "769 loss: 0.009, training_accuracy: 1.00000\n",
      "770 loss: 0.008, training_accuracy: 1.00000\n",
      "771 loss: 0.012, training_accuracy: 1.00000\n",
      "772 loss: 0.010, training_accuracy: 1.00000\n",
      "773 loss: 0.009, training_accuracy: 1.00000\n",
      "774 loss: 0.010, training_accuracy: 1.00000\n",
      "775 loss: 0.007, training_accuracy: 1.00000\n",
      "776 loss: 0.013, training_accuracy: 1.00000\n",
      "777 loss: 0.011, training_accuracy: 1.00000\n",
      "778 loss: 0.008, training_accuracy: 1.00000\n",
      "779 loss: 0.009, training_accuracy: 1.00000\n",
      "780 loss: 0.010, training_accuracy: 1.00000\n",
      "781 loss: 0.010, training_accuracy: 1.00000\n",
      "782 loss: 0.009, training_accuracy: 1.00000\n",
      "783 loss: 0.010, training_accuracy: 1.00000\n",
      "784 loss: 0.009, training_accuracy: 1.00000\n",
      "785 loss: 0.009, training_accuracy: 1.00000\n",
      "786 loss: 0.010, training_accuracy: 1.00000\n",
      "787 loss: 0.011, training_accuracy: 1.00000\n",
      "788 loss: 0.010, training_accuracy: 1.00000\n",
      "789 loss: 0.010, training_accuracy: 1.00000\n",
      "790 loss: 0.010, training_accuracy: 1.00000\n",
      "791 loss: 0.009, training_accuracy: 1.00000\n",
      "792 loss: 0.008, training_accuracy: 1.00000\n",
      "793 loss: 0.010, training_accuracy: 1.00000\n",
      "794 loss: 0.010, training_accuracy: 1.00000\n",
      "795 loss: 0.009, training_accuracy: 1.00000\n",
      "796 loss: 0.010, training_accuracy: 1.00000\n",
      "797 loss: 0.010, training_accuracy: 1.00000\n",
      "798 loss: 0.009, training_accuracy: 1.00000\n",
      "799 loss: 0.008, training_accuracy: 1.00000\n",
      "800 loss: 0.009, training_accuracy: 1.00000\n",
      "801 loss: 0.010, training_accuracy: 1.00000\n",
      "802 loss: 0.010, training_accuracy: 1.00000\n",
      "803 loss: 0.011, training_accuracy: 1.00000\n",
      "804 loss: 0.010, training_accuracy: 1.00000\n",
      "805 loss: 0.011, training_accuracy: 1.00000\n",
      "806 loss: 0.010, training_accuracy: 1.00000\n",
      "807 loss: 0.009, training_accuracy: 1.00000\n",
      "808 loss: 0.010, training_accuracy: 1.00000\n",
      "809 loss: 0.010, training_accuracy: 1.00000\n",
      "810 loss: 0.009, training_accuracy: 1.00000\n",
      "811 loss: 0.010, training_accuracy: 1.00000\n",
      "812 loss: 0.008, training_accuracy: 1.00000\n",
      "813 loss: 0.010, training_accuracy: 1.00000\n",
      "814 loss: 0.011, training_accuracy: 1.00000\n",
      "815 loss: 0.009, training_accuracy: 1.00000\n",
      "816 loss: 0.009, training_accuracy: 1.00000\n",
      "817 loss: 0.009, training_accuracy: 1.00000\n",
      "818 loss: 0.008, training_accuracy: 1.00000\n",
      "819 loss: 0.008, training_accuracy: 1.00000\n",
      "820 loss: 0.011, training_accuracy: 1.00000\n",
      "821 loss: 0.010, training_accuracy: 1.00000\n",
      "822 loss: 0.010, training_accuracy: 1.00000\n",
      "823 loss: 0.008, training_accuracy: 1.00000\n",
      "824 loss: 0.010, training_accuracy: 1.00000\n",
      "825 loss: 0.009, training_accuracy: 1.00000\n",
      "826 loss: 0.012, training_accuracy: 1.00000\n",
      "827 loss: 0.011, training_accuracy: 1.00000\n",
      "828 loss: 0.011, training_accuracy: 1.00000\n",
      "829 loss: 0.009, training_accuracy: 1.00000\n",
      "830 loss: 0.009, training_accuracy: 1.00000\n",
      "831 loss: 0.007, training_accuracy: 1.00000\n",
      "832 loss: 0.009, training_accuracy: 1.00000\n",
      "833 loss: 0.008, training_accuracy: 1.00000\n",
      "834 loss: 0.010, training_accuracy: 1.00000\n",
      "835 loss: 0.008, training_accuracy: 1.00000\n",
      "836 loss: 0.008, training_accuracy: 1.00000\n",
      "837 loss: 0.009, training_accuracy: 1.00000\n",
      "838 loss: 0.008, training_accuracy: 1.00000\n",
      "839 loss: 0.009, training_accuracy: 1.00000\n",
      "840 loss: 0.011, training_accuracy: 1.00000\n",
      "841 loss: 0.008, training_accuracy: 1.00000\n",
      "842 loss: 0.007, training_accuracy: 1.00000\n",
      "843 loss: 0.010, training_accuracy: 1.00000\n",
      "844 loss: 0.009, training_accuracy: 1.00000\n",
      "845 loss: 0.008, training_accuracy: 1.00000\n",
      "846 loss: 0.012, training_accuracy: 1.00000\n",
      "847 loss: 0.007, training_accuracy: 1.00000\n",
      "848 loss: 0.009, training_accuracy: 1.00000\n",
      "849 loss: 0.007, training_accuracy: 1.00000\n",
      "850 loss: 0.008, training_accuracy: 1.00000\n",
      "851 loss: 0.011, training_accuracy: 1.00000\n",
      "852 loss: 0.008, training_accuracy: 1.00000\n",
      "853 loss: 0.010, training_accuracy: 1.00000\n",
      "854 loss: 0.010, training_accuracy: 1.00000\n",
      "855 loss: 0.009, training_accuracy: 1.00000\n",
      "856 loss: 0.009, training_accuracy: 1.00000\n",
      "857 loss: 0.009, training_accuracy: 1.00000\n",
      "858 loss: 0.011, training_accuracy: 1.00000\n",
      "859 loss: 0.008, training_accuracy: 1.00000\n",
      "860 loss: 0.008, training_accuracy: 1.00000\n",
      "861 loss: 0.008, training_accuracy: 1.00000\n",
      "862 loss: 0.010, training_accuracy: 1.00000\n",
      "863 loss: 0.009, training_accuracy: 1.00000\n",
      "864 loss: 0.009, training_accuracy: 1.00000\n",
      "865 loss: 0.010, training_accuracy: 1.00000\n",
      "866 loss: 0.009, training_accuracy: 1.00000\n",
      "867 loss: 0.010, training_accuracy: 1.00000\n",
      "868 loss: 0.010, training_accuracy: 1.00000\n",
      "869 loss: 0.009, training_accuracy: 1.00000\n",
      "870 loss: 0.008, training_accuracy: 1.00000\n",
      "871 loss: 0.008, training_accuracy: 1.00000\n",
      "872 loss: 0.008, training_accuracy: 1.00000\n",
      "873 loss: 0.009, training_accuracy: 1.00000\n",
      "874 loss: 0.007, training_accuracy: 1.00000\n",
      "875 loss: 0.009, training_accuracy: 1.00000\n",
      "876 loss: 0.008, training_accuracy: 1.00000\n",
      "877 loss: 0.009, training_accuracy: 1.00000\n",
      "878 loss: 0.008, training_accuracy: 1.00000\n",
      "879 loss: 0.010, training_accuracy: 1.00000\n",
      "880 loss: 0.009, training_accuracy: 1.00000\n",
      "881 loss: 0.007, training_accuracy: 1.00000\n",
      "882 loss: 0.008, training_accuracy: 1.00000\n",
      "883 loss: 0.008, training_accuracy: 1.00000\n",
      "884 loss: 0.008, training_accuracy: 1.00000\n",
      "885 loss: 0.008, training_accuracy: 1.00000\n",
      "886 loss: 0.012, training_accuracy: 1.00000\n",
      "887 loss: 0.008, training_accuracy: 1.00000\n",
      "888 loss: 0.009, training_accuracy: 1.00000\n",
      "889 loss: 0.009, training_accuracy: 1.00000\n",
      "890 loss: 0.010, training_accuracy: 1.00000\n",
      "891 loss: 0.008, training_accuracy: 1.00000\n",
      "892 loss: 0.007, training_accuracy: 1.00000\n",
      "893 loss: 0.009, training_accuracy: 1.00000\n",
      "894 loss: 0.009, training_accuracy: 1.00000\n",
      "895 loss: 0.010, training_accuracy: 1.00000\n",
      "896 loss: 0.007, training_accuracy: 1.00000\n",
      "897 loss: 0.008, training_accuracy: 1.00000\n",
      "898 loss: 0.008, training_accuracy: 1.00000\n",
      "899 loss: 0.008, training_accuracy: 1.00000\n",
      "900 loss: 0.008, training_accuracy: 1.00000\n",
      "901 loss: 0.009, training_accuracy: 1.00000\n",
      "902 loss: 0.007, training_accuracy: 1.00000\n",
      "903 loss: 0.008, training_accuracy: 1.00000\n",
      "904 loss: 0.010, training_accuracy: 1.00000\n",
      "905 loss: 0.008, training_accuracy: 1.00000\n",
      "906 loss: 0.008, training_accuracy: 1.00000\n",
      "907 loss: 0.007, training_accuracy: 1.00000\n",
      "908 loss: 0.007, training_accuracy: 1.00000\n",
      "909 loss: 0.008, training_accuracy: 1.00000\n",
      "910 loss: 0.007, training_accuracy: 1.00000\n",
      "911 loss: 0.008, training_accuracy: 1.00000\n",
      "912 loss: 0.008, training_accuracy: 1.00000\n",
      "913 loss: 0.008, training_accuracy: 1.00000\n",
      "914 loss: 0.009, training_accuracy: 1.00000\n",
      "915 loss: 0.008, training_accuracy: 1.00000\n",
      "916 loss: 0.007, training_accuracy: 1.00000\n",
      "917 loss: 0.009, training_accuracy: 1.00000\n",
      "918 loss: 0.009, training_accuracy: 1.00000\n",
      "919 loss: 0.009, training_accuracy: 1.00000\n",
      "920 loss: 0.010, training_accuracy: 1.00000\n",
      "921 loss: 0.008, training_accuracy: 1.00000\n",
      "922 loss: 0.008, training_accuracy: 1.00000\n",
      "923 loss: 0.007, training_accuracy: 1.00000\n",
      "924 loss: 0.009, training_accuracy: 1.00000\n",
      "925 loss: 0.008, training_accuracy: 1.00000\n",
      "926 loss: 0.007, training_accuracy: 1.00000\n",
      "927 loss: 0.006, training_accuracy: 1.00000\n",
      "928 loss: 0.011, training_accuracy: 1.00000\n",
      "929 loss: 0.009, training_accuracy: 1.00000\n",
      "930 loss: 0.009, training_accuracy: 1.00000\n",
      "931 loss: 0.007, training_accuracy: 1.00000\n",
      "932 loss: 0.008, training_accuracy: 1.00000\n",
      "933 loss: 0.008, training_accuracy: 1.00000\n",
      "934 loss: 0.010, training_accuracy: 1.00000\n",
      "935 loss: 0.006, training_accuracy: 1.00000\n",
      "936 loss: 0.006, training_accuracy: 1.00000\n",
      "937 loss: 0.009, training_accuracy: 1.00000\n",
      "938 loss: 0.009, training_accuracy: 1.00000\n",
      "939 loss: 0.008, training_accuracy: 1.00000\n",
      "940 loss: 0.008, training_accuracy: 1.00000\n",
      "941 loss: 0.007, training_accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942 loss: 0.009, training_accuracy: 1.00000\n",
      "943 loss: 0.009, training_accuracy: 1.00000\n",
      "944 loss: 0.008, training_accuracy: 1.00000\n",
      "945 loss: 0.007, training_accuracy: 1.00000\n",
      "946 loss: 0.010, training_accuracy: 1.00000\n",
      "947 loss: 0.009, training_accuracy: 1.00000\n",
      "948 loss: 0.007, training_accuracy: 1.00000\n",
      "949 loss: 0.007, training_accuracy: 1.00000\n",
      "950 loss: 0.008, training_accuracy: 1.00000\n",
      "951 loss: 0.010, training_accuracy: 1.00000\n",
      "952 loss: 0.010, training_accuracy: 1.00000\n",
      "953 loss: 0.007, training_accuracy: 1.00000\n",
      "954 loss: 0.009, training_accuracy: 1.00000\n",
      "955 loss: 0.009, training_accuracy: 1.00000\n",
      "956 loss: 0.009, training_accuracy: 1.00000\n",
      "957 loss: 0.009, training_accuracy: 1.00000\n",
      "958 loss: 0.008, training_accuracy: 1.00000\n",
      "959 loss: 0.007, training_accuracy: 1.00000\n",
      "960 loss: 0.007, training_accuracy: 1.00000\n",
      "961 loss: 0.009, training_accuracy: 1.00000\n",
      "962 loss: 0.008, training_accuracy: 1.00000\n",
      "963 loss: 0.010, training_accuracy: 1.00000\n",
      "964 loss: 0.007, training_accuracy: 1.00000\n",
      "965 loss: 0.008, training_accuracy: 1.00000\n",
      "966 loss: 0.009, training_accuracy: 1.00000\n",
      "967 loss: 0.007, training_accuracy: 1.00000\n",
      "968 loss: 0.009, training_accuracy: 1.00000\n",
      "969 loss: 0.007, training_accuracy: 1.00000\n",
      "970 loss: 0.008, training_accuracy: 1.00000\n",
      "971 loss: 0.007, training_accuracy: 1.00000\n",
      "972 loss: 0.008, training_accuracy: 1.00000\n",
      "973 loss: 0.007, training_accuracy: 1.00000\n",
      "974 loss: 0.008, training_accuracy: 1.00000\n",
      "975 loss: 0.008, training_accuracy: 1.00000\n",
      "976 loss: 0.008, training_accuracy: 1.00000\n",
      "977 loss: 0.009, training_accuracy: 1.00000\n",
      "978 loss: 0.009, training_accuracy: 1.00000\n",
      "979 loss: 0.007, training_accuracy: 1.00000\n",
      "980 loss: 0.009, training_accuracy: 1.00000\n",
      "981 loss: 0.009, training_accuracy: 1.00000\n",
      "982 loss: 0.007, training_accuracy: 1.00000\n",
      "983 loss: 0.006, training_accuracy: 1.00000\n",
      "984 loss: 0.007, training_accuracy: 1.00000\n",
      "985 loss: 0.008, training_accuracy: 1.00000\n",
      "986 loss: 0.009, training_accuracy: 1.00000\n",
      "987 loss: 0.007, training_accuracy: 1.00000\n",
      "988 loss: 0.006, training_accuracy: 1.00000\n",
      "989 loss: 0.008, training_accuracy: 1.00000\n",
      "990 loss: 0.007, training_accuracy: 1.00000\n",
      "991 loss: 0.008, training_accuracy: 1.00000\n",
      "992 loss: 0.009, training_accuracy: 1.00000\n",
      "993 loss: 0.007, training_accuracy: 1.00000\n",
      "994 loss: 0.009, training_accuracy: 1.00000\n",
      "995 loss: 0.007, training_accuracy: 1.00000\n",
      "996 loss: 0.007, training_accuracy: 1.00000\n",
      "997 loss: 0.007, training_accuracy: 1.00000\n",
      "998 loss: 0.011, training_accuracy: 1.00000\n",
      "999 loss: 0.006, training_accuracy: 1.00000\n",
      "1000 loss: 0.008, training_accuracy: 1.00000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for epoch in range(epochs_num):\n",
    "    # training\n",
    "    running_loss = 0.0\n",
    "    training_accuracy = 0.0\n",
    "    for i in range(int(training_size / batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, label = mkRandomBatch(train_x, train_t, batch_size)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data.item()\n",
    "        training_accuracy += np.sum(np.abs((output.data - label.data).numpy()) < 0.1) #outputとlabelの誤差が0.1以内なら正しいとみなす。\n",
    "    training_accuracy /= training_size\n",
    "    print('%d loss: %.3f, training_accuracy: %.5f' % (epoch + 1, running_loss, training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2396c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loss: 0.008, training_accuracy: 0.01000, test_accuracy: 1.00000\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "test_accuracy = 0.0\n",
    "test_size = 1000\n",
    "test_x, test_t = mkDataSet(test_size)\n",
    "\n",
    "for i in range(int(test_size / batch_size)):\n",
    "    offset = i * batch_size\n",
    "    data, label = torch.tensor(test_x[offset:offset+batch_size]), torch.tensor(test_t[offset:offset+batch_size])\n",
    "    output = model(data, None)\n",
    "\n",
    "    test_accuracy += np.sum(np.abs((output.data - label.data).numpy()) < 0.1)\n",
    "\n",
    "training_accuracy /= training_size\n",
    "test_accuracy /= test_size\n",
    "\n",
    "print('%d loss: %.3f, training_accuracy: %.5f, test_accuracy: %.5f' % (\n",
    "    epoch + 1, running_loss, training_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912762e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
