{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391eb5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pfrl\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import numpy\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f263df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stocknet.envs.bc_env import BC5Env\n",
    "from stocknet.envs.market_clients.csv.client import CSVClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68b8402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b3f53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3cd5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_client = CSVClient('../../data_source/bitcoin_5_2017T0710-2021T103022.csv')\n",
    "env = BC5Env(data_client, columns=[\"Open\", \"High\", \"Low\", \"Close\"], useBudgetColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851d8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class PredictorSimple(nn.Module):\n",
    "    def __init__(self, size, inputDim, n_actions, removeHistoryData = True):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.rhd = removeHistoryData\n",
    "        self.inDim = inputDim\n",
    "        self.ActionHistoryDim = 3\n",
    "        self.conv1 = nn.Conv1d(inputDim, inputDim*3, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(inputDim*3, inputDim*2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(inputDim*2*size, inputDim*size)\n",
    "        self.fc2 = nn.Linear(inputDim*size, size)\n",
    "        self.output_layer = nn.Linear(size, n_actions)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, feature_len, seq_len  = inputs.shape[0], inputs.shape[1],inputs.shape[2]\n",
    "        if self.rhd:\n",
    "            out = inputs[:,0: feature_len - self.ActionHistoryDim, :]\n",
    "        else:\n",
    "            out = inputs\n",
    "        out = torch.tanh(self.conv1(out))\n",
    "        out = torch.tanh(self.conv2(out))\n",
    "        out = out.view(-1, self.inDim*2*self.size)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = torch.tanh(self.fc2(out))\n",
    "        out = self.output_layer(out)\n",
    "        return pfrl.action_value.DiscreteActionValue(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b05b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 1000 #traning dataのデータ数\n",
    "epochs_num = 10000 #traningのepoch回数\n",
    "hidden_size = 500 #LSTMの隠れ層の次元数\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dec9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "inputDim = obs.shape[0]\n",
    "size = obs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd20dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f91f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PredictorSimple(size, inputDim, 3, False) #modelの宣言\n",
    "\n",
    "#model = Predictor(6, hidden_size, 5) #modelの宣言\n",
    "criterion = nn.MSELoss() #評価関数の宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91597e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGD(model.parameters(), lr=0.0001) #最適化関数の宣言\n",
    "optimizer = torch.optim.Adam(model.parameters(), eps=1e-4)\n",
    "# Set the discount factor that discounts future rewards.\n",
    "gamma = 0.9\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "explorer = pfrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.1, random_action_func=env.action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.}\n",
    "# Specify a replay buffer and its capacity.\n",
    "replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10**2)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# As PyTorch only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "phi = lambda x: x.astype(numpy.float32, copy=False)\n",
    "\n",
    "# Set the device id to use GPU. To use CPU only, set it to -1.\n",
    "gpu = 0\n",
    "\n",
    "# Now create an agent that will interact with the environment.\n",
    "agent = pfrl.agents.DoubleDQN(\n",
    "    model,\n",
    "    optimizer,\n",
    "    replay_buffer,\n",
    "    gamma,\n",
    "    explorer,\n",
    "    replay_start_size=40,\n",
    "    update_interval=5,\n",
    "    target_update_interval=20,\n",
    "    phi=phi,\n",
    "    gpu=gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7ccb9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-16 15:02:43.819698 start episodes\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 31.767239), ('average_loss', 3.699133024215698), ('cumulative_steps', 38481), ('n_updates', 7689), ('rlen', 100)] R: -0.019442876537156362 Mean R: 1.0954418741442282 PL: -1.003199271297293 Mean PL: 33688.07650264995\n",
      "pl: -1.003199271297293\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 52.23422), ('average_loss', 6.085895175933838), ('cumulative_steps', 62283), ('n_updates', 12449), ('rlen', 100)] R: -0.011851847258422242 Mean R: 1.575044573494469 PL: -7.951915339945941 Mean PL: 83541.00820695575\n",
      "pl: -7.951915339945941\n",
      "max steps\n",
      "statistics: [('average_q', 0.14172278), ('average_loss', 0.0002792327536371886), ('cumulative_steps', 91365), ('n_updates', 18266), ('rlen', 100)] R: -0.0014808287719497147 Mean R: 0.30175289874105715 PL: -1.0770170204134135 Mean PL: 90077.56734440688\n",
      "pl: -1.0770170204134135\n",
      "max steps\n",
      "statistics: [('average_q', 33.415085), ('average_loss', 2.8401054835319517), ('cumulative_steps', 106410), ('n_updates', 21275), ('rlen', 100)] R: -0.005919549053665371 Mean R: 0.41205934093740637 PL: -1.0045052169860798 Mean PL: 101926.64216681388\n",
      "pl: -1.0045052169860798\n",
      "max steps\n",
      "statistics: [('average_q', 5.764205), ('average_loss', 0.18927619211375712), ('cumulative_steps', 131389), ('n_updates', 26270), ('rlen', 100)] R: -0.004759653676083727 Mean R: 0.21054135595931664 PL: -1.1511158935080568 Mean PL: 107902.63514822573\n",
      "pl: -1.1511158935080568\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 57.156307), ('average_loss', 6.6624541664123536), ('cumulative_steps', 178700), ('n_updates', 35733), ('rlen', 100)] R: -0.0066724645075795595 Mean R: 2.526958222902283 PL: -1.5712586879376884 Mean PL: 197646.58449162444\n",
      "pl: -1.5712586879376884\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 22.417683), ('average_loss', 1.790610432624817), ('cumulative_steps', 205842), ('n_updates', 41161), ('rlen', 100)] R: -0.0025879267106170122 Mean R: 1.7027489046473285 PL: -1.0629471052374002 Mean PL: 256215.2544926643\n",
      "pl: -1.0629471052374002\n",
      "max steps\n",
      "statistics: [('average_q', 70.048515), ('average_loss', 3.930101515054703), ('cumulative_steps', 236527), ('n_updates', 47298), ('rlen', 100)] R: 6.604679644625371 Mean R: 0.07493517981840228 PL: 198042.5477118371 Mean PL: 256174.47073196006\n",
      "pl: 198042.5477118371\n",
      "statistics: [('average_q', 8.882323), ('average_loss', 0.40421134263277053), ('cumulative_steps', 248409), ('n_updates', 49674), ('rlen', 100)] R: -0.008661298933119144 Mean R: 0.08381416077109115 PL: -1.4835067660088255 Mean PL: 257311.3216966954\n",
      "pl: -1.4835067660088255\n",
      "max steps\n",
      "statistics: [('average_q', 13.436325), ('average_loss', 1.0146818619966507), ('cumulative_steps', 263078), ('n_updates', 52608), ('rlen', 100)] R: -0.007648386096250281 Mean R: 0.39997906545319906 PL: -1.0299384831258205 Mean PL: 270692.20943998045\n",
      "pl: -1.0299384831258205\n",
      "statistics: [('average_q', 0.6372661), ('average_loss', 0.004445275348552968), ('cumulative_steps', 270239), ('n_updates', 54040), ('rlen', 100)] R: -0.002488132242341433 Mean R: -0.0051702557509863524 PL: -4.095868132094436 Mean PL: 270686.08879117854\n",
      "pl: -4.095868132094436\n",
      "statistics: [('average_q', 0.7282638), ('average_loss', 0.005301580960076535), ('cumulative_steps', 277785), ('n_updates', 55550), ('rlen', 100)] R: 0.010925791744264954 Mean R: 0.022769618051842876 PL: -4.622381420500044 Mean PL: 270674.42166089657\n",
      "pl: -4.622381420500044\n",
      "max steps\n",
      "statistics: [('average_q', 9.774614), ('average_loss', 0.44678782507777215), ('cumulative_steps', 298068), ('n_updates', 59606), ('rlen', 100)] R: 0.00881632291864172 Mean R: 0.6421577019913862 PL: -3.2357194411639516 Mean PL: 295421.95675068145\n",
      "pl: -3.2357194411639516\n",
      "max steps\n",
      "statistics: [('average_q', 17.204344), ('average_loss', 1.2722884917259216), ('cumulative_steps', 317072), ('n_updates', 63407), ('rlen', 100)] R: -0.015483735090611449 Mean R: 0.8165047870256575 PL: -1.1871074299282995 Mean PL: 321666.59880694613\n",
      "pl: -1.1871074299282995\n",
      "statistics: [('average_q', 0.05151429), ('average_loss', 0.00019108782827970572), ('cumulative_steps', 326222), ('n_updates', 65237), ('rlen', 100)] R: 0.0012647299446172527 Mean R: -0.004886140448536751 PL: -1.2552521598748692 Mean PL: 321662.1838127703\n",
      "pl: -1.2552521598748692\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 29.532667), ('average_loss', 1.7491706384345889), ('cumulative_steps', 368618), ('n_updates', 73716), ('rlen', 100)] R: 2.884663093441286 Mean R: 0.29949493288962237 PL: 91243.44145746013 Mean PL: 333270.249282293\n",
      "pl: 91243.44145746013\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 58.64644), ('average_loss', 4.6387318480014805), ('cumulative_steps', 405607), ('n_updates', 81114), ('rlen', 100)] R: 5.837586036615103 Mean R: 1.6368568493544502 PL: 166380.37897195396 Mean PL: 376351.13626430416\n",
      "pl: 166380.37897195396\n",
      "statistics: [('average_q', 3.718933), ('average_loss', 0.08007250275462865), ('cumulative_steps', 416897), ('n_updates', 83372), ('rlen', 100)] R: -0.023825703597406222 Mean R: 0.0026259444940085253 PL: -1.4127727223608337 Mean PL: 376338.7907763067\n",
      "pl: -1.4127727223608337\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 47.853592), ('average_loss', 4.683616149425506), ('cumulative_steps', 451977), ('n_updates', 90388), ('rlen', 100)] R: 4.007661154166135 Mean R: 0.9100037134599683 PL: 103809.27457892445 Mean PL: 401907.39300816256\n",
      "pl: 103809.27457892445\n",
      "max steps\n",
      "statistics: [('average_q', 3.9242573), ('average_loss', 1.746798963136971), ('cumulative_steps', 473044), ('n_updates', 94601), ('rlen', 100)] R: 0.001047348199499404 Mean R: 0.10989800806156849 PL: -1.232510365294391 Mean PL: 404352.00064549\n",
      "pl: -1.232510365294391\n",
      "max steps\n",
      "statistics: [('average_q', 60.88462), ('average_loss', 5.487823026180267), ('cumulative_steps', 491974), ('n_updates', 98387), ('rlen', 100)] R: -0.033513367905205385 Mean R: 0.5032407788227892 PL: -2.4392694517908504 Mean PL: 418619.36847875034\n",
      "pl: -2.4392694517908504\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 33.907017), ('average_loss', 9.044425477981568), ('cumulative_steps', 537404), ('n_updates', 107473), ('rlen', 100)] R: 2.8605616353480987 Mean R: 1.1537805230536367 PL: 100308.29690782948 Mean PL: 458373.36228901206\n",
      "pl: 100308.29690782948\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 15.715365), ('average_loss', 3.9153513860702516), ('cumulative_steps', 571215), ('n_updates', 114236), ('rlen', 100)] R: -0.01672491974082277 Mean R: 1.1655371709800042 PL: -1.4260168412607073 Mean PL: 491584.10982996476\n",
      "pl: -1.4260168412607073\n",
      "statistics: [('average_q', 1.5952705), ('average_loss', 0.2559858498629183), ('cumulative_steps', 579866), ('n_updates', 115966), ('rlen', 100)] R: 0.2036861050556385 Mean R: -0.0060664386663161415 PL: -82.31752008672615 Mean PL: 491582.35033854714\n",
      "pl: -82.31752008672615\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 42.376236), ('average_loss', 4.39403149843216), ('cumulative_steps', 617352), ('n_updates', 123463), ('rlen', 100)] R: -0.035104971618060615 Mean R: 1.1457319372170247 PL: -4.174999002674484 Mean PL: 530490.7618890641\n",
      "pl: -4.174999002674484\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 1.3615234), ('average_loss', 0.37879442794015633), ('cumulative_steps', 653479), ('n_updates', 130688), ('rlen', 100)] R: 0.17371019861406178 Mean R: 1.5134164652129298 PL: -183.4472136686738 Mean PL: 575387.1647561144\n",
      "pl: -183.4472136686738\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 43.300583), ('average_loss', 3.7324811482429503), ('cumulative_steps', 701893), ('n_updates', 140371), ('rlen', 100)] R: -0.0006808329607396187 Mean R: 2.4566076020127863 PL: -1.300959131832999 Mean PL: 653797.4518112902\n",
      "pl: -1.300959131832999\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 20.48064), ('average_loss', 1.8337468218803405), ('cumulative_steps', 738872), ('n_updates', 147767), ('rlen', 100)] R: 0.0021111020019341127 Mean R: 1.4717274432578749 PL: -2.047266696447154 Mean PL: 700525.074831648\n",
      "pl: -2.047266696447154\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 79.04839), ('average_loss', 2.874921888709068), ('cumulative_steps', 777159), ('n_updates', 155424), ('rlen', 100)] R: 7.055817554628707 Mean R: 2.401045431071664 PL: 241453.53239998044 Mean PL: 773726.601665961\n",
      "pl: 241453.53239998044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max steps\n",
      "statistics: [('average_q', 16.425484), ('average_loss', 1.192324856519699), ('cumulative_steps', 797678), ('n_updates', 159528), ('rlen', 100)] R: -0.0034147670785224897 Mean R: 0.19861003857973752 PL: -1.2109794838637773 Mean PL: 781185.7237962111\n",
      "pl: -1.2109794838637773\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 42.944096), ('average_loss', 5.816650772094727), ('cumulative_steps', 842691), ('n_updates', 168531), ('rlen', 100)] R: 4.296956452594272 Mean R: 1.1496184731671397 PL: 139141.87127407407 Mean PL: 814484.040848593\n",
      "pl: 139141.87127407407\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 37.161118), ('average_loss', 3.234438171386719), ('cumulative_steps', 877409), ('n_updates', 175474), ('rlen', 100)] R: -0.00203714998718376 Mean R: 1.6643493558379536 PL: -1.2616271241567278 Mean PL: 863773.6777722139\n",
      "pl: -1.2616271241567278\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 9.384017), ('average_loss', 0.46726679474115373), ('cumulative_steps', 905847), ('n_updates', 181162), ('rlen', 100)] R: -0.011223410585950057 Mean R: 0.9389539871677345 PL: -20.677369945882322 Mean PL: 892360.9891387541\n",
      "pl: -20.677369945882322\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 5.455572), ('average_loss', 0.3038972458988428), ('cumulative_steps', 949725), ('n_updates', 189938), ('rlen', 100)] R: 0.09525356526725764 Mean R: 1.7353574753679382 PL: -11.446990876975352 Mean PL: 938885.0661695737\n",
      "pl: -11.446990876975352\n",
      "max steps\n",
      "statistics: [('average_q', 22.6397), ('average_loss', 1.8028486597537994), ('cumulative_steps', 981243), ('n_updates', 196241), ('rlen', 100)] R: -0.006582504092504066 Mean R: 0.9322746130673549 PL: -1.1494734329434797 Mean PL: 963662.8025591373\n",
      "pl: -1.1494734329434797\n",
      "max steps\n",
      "statistics: [('average_q', 37.927593), ('average_loss', 3.3867453598976134), ('cumulative_steps', 1005193), ('n_updates', 201031), ('rlen', 100)] R: 0.01893073749819322 Mean R: 0.7091582572069379 PL: -5.119920588396548 Mean PL: 984427.033008971\n",
      "pl: -5.119920588396548\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 44.242016), ('average_loss', 4.455195951461792), ('cumulative_steps', 1040684), ('n_updates', 208129), ('rlen', 100)] R: -0.0004888078964545078 Mean R: 0.8108585942482789 PL: -1.0834300729694941 Mean PL: 1009321.9656781049\n",
      "pl: -1.0834300729694941\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 42.534435), ('average_loss', 8.439586954116821), ('cumulative_steps', 1093391), ('n_updates', 218671), ('rlen', 100)] R: 3.2339967263119096 Mean R: 1.363465138283758 PL: 81952.58756158684 Mean PL: 1047976.498231604\n",
      "pl: 81952.58756158684\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 19.00378), ('average_loss', 2.14373024225235), ('cumulative_steps', 1132852), ('n_updates', 226563), ('rlen', 100)] R: 0.02756455129663004 Mean R: 1.3263086107551127 PL: -55.919818064061054 Mean PL: 1086895.7537878375\n",
      "pl: -55.919818064061054\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 8.317847), ('average_loss', 0.37095913499593736), ('cumulative_steps', 1165593), ('n_updates', 233111), ('rlen', 100)] R: -0.006202462593420786 Mean R: 0.6176880816621105 PL: -1.1279450103813522 Mean PL: 1106300.0427341922\n",
      "pl: -1.1279450103813522\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 42.464336), ('average_loss', 3.7900019025802614), ('cumulative_steps', 1189738), ('n_updates', 237940), ('rlen', 100)] R: -0.04895509727278902 Mean R: 1.3497261121705924 PL: -2.6058821043618874 Mean PL: 1144327.0630109694\n",
      "pl: -2.6058821043618874\n",
      "max steps\n",
      "statistics: [('average_q', 31.333681), ('average_loss', 32.98634714126587), ('cumulative_steps', 1204762), ('n_updates', 240945), ('rlen', 100)] R: 6.077114957561913 Mean R: -0.015039308406002363 PL: 182239.42130162692 Mean PL: 1144325.790055241\n",
      "pl: 182239.42130162692\n",
      "max steps\n",
      "statistics: [('average_q', 15.907721), ('average_loss', 1.27676493704319), ('cumulative_steps', 1227712), ('n_updates', 245535), ('rlen', 100)] R: -0.027462471882751197 Mean R: 0.1686619795352119 PL: -2.022825881322083 Mean PL: 1149730.08623864\n",
      "pl: -2.022825881322083\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 54.86669), ('average_loss', 5.050310306549072), ('cumulative_steps', 1261601), ('n_updates', 252313), ('rlen', 100)] R: -0.004934020570203051 Mean R: 1.8916200357883544 PL: -1.3714264586806109 Mean PL: 1218258.045033179\n",
      "pl: -1.3714264586806109\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 2.4708364), ('average_loss', 0.29501399768632836), ('cumulative_steps', 1295235), ('n_updates', 259040), ('rlen', 100)] R: 0.15482977054508792 Mean R: 0.2473627837336289 PL: -37.57049661218335 Mean PL: 1224107.861333583\n",
      "pl: -37.57049661218335\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 52.650063), ('average_loss', 2.7991661754250527), ('cumulative_steps', 1319981), ('n_updates', 263989), ('rlen', 100)] R: 5.929582043700904 Mean R: 0.2105763195844052 PL: 152018.13652122713 Mean PL: 1231600.963177192\n",
      "pl: 152018.13652122713\n",
      "max steps\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 24.854635), ('average_loss', 1.9668097412586212), ('cumulative_steps', 1361447), ('n_updates', 272282), ('rlen', 100)] R: -0.020468225945275715 Mean R: 2.1206650105079943 PL: -6.92266253534233 Mean PL: 1303111.425824265\n",
      "pl: -6.92266253534233\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 42.487522), ('average_loss', 4.793360266685486), ('cumulative_steps', 1391240), ('n_updates', 278241), ('rlen', 100)] R: -0.0020102474486316142 Mean R: 1.25968329437439 PL: -2.037936983324457 Mean PL: 1341428.4747632544\n",
      "pl: -2.037936983324457\n",
      "statistics: [('average_q', 1.5499694), ('average_loss', 0.2700916811265051), ('cumulative_steps', 1406602), ('n_updates', 281313), ('rlen', 100)] R: -0.074950001246388 Mean R: 0.03760452636911778 PL: -26.980292827784364 Mean PL: 1341383.156336706\n",
      "pl: -26.980292827784364\n",
      "max steps\n",
      "max steps\n",
      "statistics: [('average_q', 1.4697949), ('average_loss', 0.0970071407360956), ('cumulative_steps', 1437841), ('n_updates', 287561), ('rlen', 100)] R: -0.0053656442146738 Mean R: 1.3638026862689248 PL: -1.4382440671016434 Mean PL: 1384831.5151591809\n",
      "pl: -1.4382440671016434\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 500\n",
    "max_step_len = 10000\n",
    "mr = 0\n",
    "pl = 0\n",
    "print(datetime.datetime.now(),'start episodes')\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    #obs = obs.to('cpu').detach().numpy().copy()\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while True:\n",
    "        # Uncomment to watch the behavior in a GUI window\n",
    "        #env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, ops = env.step(action)\n",
    "        #obs = obs.to('cpu').detach().numpy().copy()\n",
    "        R += reward\n",
    "        t += 1\n",
    "        reset = t == max_step_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        if reset:\n",
    "            print(\"max steps\")\n",
    "            break\n",
    "        elif done:\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print('statistics:', agent.get_statistics(), 'R:', R/t, 'Mean R:', mr/10, 'PL:', env.pl, 'Mean PL:', pl/10)\n",
    "        env.render()\n",
    "        mr = 0\n",
    "        pi = 0\n",
    "    else:\n",
    "        pl += env.pl\n",
    "        mr += R/t\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf9b51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bc_rl_5min_macd-bolinger_with_budgets_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc236747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('fx_rl_5min_macd_v1', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd809e4",
   "metadata": {},
   "source": [
    "### DoubleDQN with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de72ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_client = CSVClient()\n",
    "env = BC5Env(data_client, columns=[\"macd\"], useBudgetColumns=True, featureFirst=False,use_diff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4b49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 1000 #traning dataのデータ数\n",
    "epochs_num = 10000 #traningのepoch回数\n",
    "hidden_size = 500 #LSTMの隠れ層の次元数\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c292a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "size = obs.shape[0]\n",
    "inputDim = obs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5580019f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6239948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class PredictorLSTM(nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim,  n_actions):\n",
    "        super(PredictorLSTM, self).__init__()\n",
    "        self.ActionHistoryDim = 2\n",
    "        self.rnn = nn.LSTM(input_size = inputDim - self.ActionHistoryDim,\n",
    "                            hidden_size = hiddenDim,\n",
    "                            batch_first=True)\n",
    "        self.rnn.to(device)\n",
    "        self.output_layer = nn.Linear(hiddenDim, n_actions)#+self.ActionHistoryDim, n_actions)\n",
    "        self.output_layer.to(device)\n",
    "    \n",
    "    def forward(self, inputs, hidden0=None):\n",
    "        batch_size, seq_len, feature_len = inputs.shape[0], inputs.shape[1],inputs.shape[2]\n",
    "        ohlc_inputs = inputs[:,:, 0: feature_len - self.ActionHistoryDim]\n",
    "        last_actions = inputs[:, -1, -self.ActionHistoryDim:] # [1, ActionHistoryDim] (ex.torch.Size([1, 3]))\n",
    "        output, (hidden, cell) = self.rnn(ohlc_inputs, hidden0) #LSTM層\n",
    "        output = output[:, -1, :] # [1, hidden_size] (ex. torch.Size([1, 50]))\n",
    "        #output = torch.cat((output, last_actions), dim=1) #[1, hidden_size+ActionHistoryDim] (ex.torch.Size([1, 53]))\n",
    "        output = self.output_layer(output) #全結合層\n",
    "        return pfrl.action_value.DiscreteActionValue(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00bf49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PredictorLSTM(inputDim, 50, 3) #modelの宣言\n",
    "criterion = nn.MSELoss() #評価関数の宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb07c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGD(model.parameters(), lr=0.0001) #最適化関数の宣言\n",
    "optimizer = torch.optim.Adam(model.parameters(), eps=1e-3)\n",
    "# Set the discount factor that discounts future rewards.\n",
    "gamma = 0.9\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "explorer = pfrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.1, random_action_func=env.action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10**2)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# As PyTorch only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "phi = lambda x: x.astype(numpy.float32, copy=False)\n",
    "\n",
    "# Set the device id to use GPU. To use CPU only, set it to -1.\n",
    "gpu = 0\n",
    "\n",
    "# Now create an agent that will interact with the environment.\n",
    "agent = pfrl.agents.DoubleDQN(\n",
    "    model,\n",
    "    optimizer,\n",
    "    replay_buffer,\n",
    "    gamma,\n",
    "    explorer,\n",
    "    replay_start_size=40,\n",
    "    update_interval=5,\n",
    "    target_update_interval=20,\n",
    "    phi=phi,\n",
    "    gpu=gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e23b2ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:51:43.079519 start episodes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4023435131ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_step_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max steps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pfrl/agent.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, obs, reward, done, reset)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_observe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pfrl/agents/dqn.py\u001b[0m in \u001b[0;36mbatch_observe\u001b[0;34m(self, batch_obs, batch_reward, batch_done, batch_reset)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             return self._batch_observe_train(\n\u001b[0;32m--> 587\u001b[0;31m                 \u001b[0mbatch_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m             )\n\u001b[1;32m    589\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pfrl/agents/dqn.py\u001b[0m in \u001b[0;36m_batch_observe_train\u001b[0;34m(self, batch_obs, batch_reward, batch_done, batch_reset)\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_last_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_current_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_updater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_if_necessary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pfrl/replay_buffer.py\u001b[0m in \u001b[0;36mupdate_if_necessary\u001b[0;34m(self, iteration)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pfrl/agents/dqn.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, experiences, errors_out)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mpfrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_l2_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_episodes = 5000\n",
    "max_step_len = 10000\n",
    "mr = 0\n",
    "pl = 0\n",
    "print(datetime.datetime.now(),'start episodes')\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    #obs = obs.to('cpu').detach().numpy().copy()\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while True:\n",
    "        # Uncomment to watch the behavior in a GUI window\n",
    "        #env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, ops = env.step(action)\n",
    "        #obs = obs.to('cpu').detach().numpy().copy()\n",
    "        R += reward\n",
    "        t += 1\n",
    "        reset = t == max_step_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        if reset:\n",
    "            print(\"max steps\")\n",
    "            break\n",
    "        elif done:\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print('statistics:', agent.get_statistics(), 'R:', R/t, 'Mean R:', mr/10, 'PL:', env.pl, 'Mean PL:', pl/10)\n",
    "        mr = 0\n",
    "        pl = 0\n",
    "    else:\n",
    "        pl += env.pl\n",
    "        mr += R/t\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993fd64",
   "metadata": {},
   "source": [
    "結果\n",
    "\n",
    "損益がプラスの場合でもRewardがマイナスになっている。\n",
    "Mean R: -350.2810497539654 PL: -25455.236926080022 Mean PL: 582725.8480107023\n",
    "\n",
    "これよりも高いRewardだが、損益はマイナスとなるケースがある\n",
    "Mean R: -192.19638108751334 PL: 2673622.0302433698 Mean PL: -19351.08942841183\n",
    "\n",
    "stayによるrewardか、invalid_rewardのどちらか、あるいは両方が影響しているため、期待どおりの報酬となるように調査と修正が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca496225",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bc_rl_5min_macd_with_budget_LSTM_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f45157d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('bc_rl_5min_macd_with_budget_LSTM_v1', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c6d41",
   "metadata": {},
   "source": [
    "改善点\n",
    "\n",
    "hidden_sizeは隠れ層のFeatureSizeでござった\n",
    "\n",
    "Task 1: num_layersが隠れ層の数なので、こちらを増やしてみる\n",
    "\n",
    "\n",
    "replay bufferはバッファーされたExperience(state, action, rewards)からランダムにサンプリングして学習に用いる\n",
    "このためLSTMのように前回の出力を入力に用いるモデルとは相性が悪いと思われる\n",
    "\n",
    "ゲームのような場合、直ぐに詰んで同じ状況ばかりが入力になるので、Replay bufferによって入力をランダムにすることで、入力の状態がばらけて学習が安定するのだろうが、reset毎に初期位置をランダム化している現在のEnvの実装では、Replay bufferが無いほうがLSTMとしては都合が良さそうである\n",
    "\n",
    "Task 2:Replay Bufferを無効にする方法はないだろうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fce3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_client = CSVClient()\n",
    "env = BC5Env(data_client, columns=[\"macd\"], useBudgetColumns=True, featureFirst=False,use_diff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd29945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class PredictorLSTM(nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, num_layers, n_actions):\n",
    "        super(PredictorLSTM, self).__init__()\n",
    "        self.ActionHistoryDim = 2\n",
    "        self.rnn = nn.LSTM(input_size = inputDim - self.ActionHistoryDim,\n",
    "                            hidden_size = hiddenDim,\n",
    "                            batch_first=True,\n",
    "                            num_layers=num_layers)\n",
    "        self.rnn.to(device)\n",
    "        self.output_layer = nn.Linear(hiddenDim, n_actions)#+self.ActionHistoryDim, n_actions)\n",
    "        self.output_layer.to(device)\n",
    "    \n",
    "    def forward(self, inputs, hidden0=None):\n",
    "        batch_size, seq_len, feature_len = inputs.shape[0], inputs.shape[1],inputs.shape[2]\n",
    "        ohlc_inputs = inputs[:,:, 0: feature_len - self.ActionHistoryDim]\n",
    "        last_actions = inputs[:, -1, -self.ActionHistoryDim:] # [1, ActionHistoryDim] (ex.torch.Size([1, 3]))\n",
    "        output, (hidden, cell) = self.rnn(ohlc_inputs, hidden0) #LSTM層\n",
    "        output = output[:, -1, :] # [1, hidden_size] (ex. torch.Size([1, 50]))\n",
    "        #output = torch.cat((output, last_actions), dim=1) #[1, hidden_size+ActionHistoryDim] (ex.torch.Size([1, 53]))\n",
    "        output = self.output_layer(output) #全結合層\n",
    "        return pfrl.action_value.DiscreteActionValue(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9785fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PredictorLSTM(inputDim, inputDim,30, 3) #modelの宣言\n",
    "criterion = nn.MSELoss() #評価関数の宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0d70f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a11d031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGD(model.parameters(), lr=0.0001) #最適化関数の宣言\n",
    "optimizer = torch.optim.Adam(model.parameters(), eps=1e-3)\n",
    "# Set the discount factor that discounts future rewards.\n",
    "gamma = 0.9\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "explorer = pfrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.1, random_action_func=env.action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "#replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=batch_size)\n",
    "replay_buffer = pfrl.replay_buffers.EpisodicReplayBuffer(capacity=batch_size)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# As PyTorch only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "phi = lambda x: x.astype(numpy.float32, copy=False)\n",
    "\n",
    "# Set the device id to use GPU. To use CPU only, set it to -1.\n",
    "gpu = 0\n",
    "\n",
    "# Now create an agent that will interact with the environment.\n",
    "agent = pfrl.agents.DoubleDQN(\n",
    "    model,\n",
    "    optimizer,\n",
    "    replay_buffer,\n",
    "    gamma,\n",
    "    explorer,\n",
    "    minibatch_size=batch_size,\n",
    "    replay_start_size=batch_size,\n",
    "    update_interval=1,\n",
    "    target_update_interval=100,\n",
    "    phi=phi,\n",
    "    gpu=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4521b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-29 19:57:36.389617 start episodes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aa77baaabf78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Uncomment to watch the behavior in a GUI window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#obs = obs.to('cpu').detach().numpy().copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "n_episodes = 5000\n",
    "max_step_len = 10000\n",
    "mr = 0\n",
    "pl = 0\n",
    "print(datetime.datetime.now(),'start episodes')\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    #obs = obs.to('cpu').detach().numpy().copy()\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while True:\n",
    "        # Uncomment to watch the behavior in a GUI window\n",
    "        #env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, ops = env.step(action)\n",
    "        #obs = obs.to('cpu').detach().numpy().copy()\n",
    "        R += reward\n",
    "        t += 1\n",
    "        reset = t == max_step_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        if reset:\n",
    "            print(\"max steps\")\n",
    "            break\n",
    "        elif done:\n",
    "            break\n",
    "    pl += env.pl\n",
    "    mr += R/t\n",
    "    if i % 10 == 0:\n",
    "        print('statistics:', agent.get_statistics(), 'R:', R/t, 'Mean R:', mr/10, 'PL:', env.pl, 'Mean PL:', pl/10)\n",
    "        mr = 0\n",
    "        pl = 0\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff298b34",
   "metadata": {},
   "source": [
    "改善点\n",
    "\n",
    "replay bufferはバッファーされたExperience(state, action, rewards)からランダムにサンプリングして学習に用いる\n",
    "このためLSTMのように前回の出力を入力に用いるモデルとは相性が悪いと思われる\n",
    "\n",
    "ゲームのような場合、直ぐに詰んで同じ状況ばかりが入力になるので、Replay bufferによって入力をランダムにすることで、入力の状態がばらけて学習が安定するのだろうが、reset毎に初期位置をランダム化している現在のEnvの実装では、Replay bufferが無いほうがLSTMとしては都合が良さそうである\n",
    "\n",
    "Task 1: LSTMと同じだけの隠れ層（30）を持つ全結合グラフのモデルを試してLSTMの結果と比較する\n",
    "MACDの場合、既に平滑化されているためConv層がいるか微妙であり、またLSTMのグラフもConv層はないはずなので、Conv層無しで試してみる。\n",
    "\n",
    "Task 2:OHLCの差分配列にした際、値は-1から1の範囲に収まっているが、最大値は0.5程度となっており、他の指標（Volume等）と比べて、0.5の値の持つ意味（あるいは重み）が異なっていた。\n",
    "MACDの場合はどうなっているか確認し、値の範囲が適切でなければ正規化を追加で、あるいは差分をやめて正規化のみを適用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239b3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_client = CSVClient('../../data_source/bitcoin_5_2017T0710-2021T103022.csv')\n",
    "env = BC5Env(data_client, columns=[\"macd\"], useBudgetColumns=True, use_diff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab62bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class PredictorMultiple(nn.Module):\n",
    "    def __init__(self, layer_num, size, inputDim, n_actions, removeHistoryData = True):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.rhd = removeHistoryData\n",
    "        self.ActionHistoryDim = 2\n",
    "        if removeHistoryData:\n",
    "            input_dims = inputDim - self.ActionHistoryDim\n",
    "        else:\n",
    "            input_dims = inputDim\n",
    "        self.layerDips = size * input_dims\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(0, layer_num):\n",
    "            layer = nn.Linear(self.layerDips, self.layerDips)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        out_in_dims = self.layerDips\n",
    "        if self.rhd:\n",
    "            out_in_dims += self.ActionHistoryDim\n",
    "        self.output_layer = nn.Linear( out_in_dims , n_actions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, feature_len,seq_len = inputs.shape[0], inputs.shape[1],inputs.shape[2]\n",
    "        if self.rhd:\n",
    "            feature_len = feature_len - self.ActionHistoryDim\n",
    "            last_actions = inputs[:, -self.ActionHistoryDim:,-1] # [1, ActionHistoryDim] (ex.torch.Size([1, 3]))\n",
    "        out = inputs[:, :feature_len, :]\n",
    "        layerDips = feature_len * seq_len\n",
    "        #assert layerDips == self.layerDips\n",
    "        out = out.view(-1, layerDips)\n",
    "        for layer in self.layers:\n",
    "            out = torch.tanh(layer(out))\n",
    "        if self.rhd:\n",
    "            out = torch.cat((out, last_actions), dim=1)\n",
    "        out = torch.tanh(self.output_layer(out))\n",
    "        return pfrl.action_value.DiscreteActionValue(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f6cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "inputDim, size = obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f434857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abb4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e55d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PredictorMultiple(30,size, inputDim, 3, removeHistoryData=False) #modelの宣言\n",
    "criterion = nn.MSELoss() #評価関数の宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4c5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGD(model.parameters(), lr=0.0001) #最適化関数の宣言\n",
    "optimizer = torch.optim.Adam(model.parameters(), eps=1e-7)\n",
    "# Set the discount factor that discounts future rewards.\n",
    "gamma = 0.9\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "explorer = pfrl.explorers.ConstantEpsilonGreedy(epsilon=0.1, random_action_func=env.action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "#replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=batch_size)\n",
    "replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=batch_size)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# As PyTorch only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "phi = lambda x: x.astype(numpy.float32, copy=False)\n",
    "\n",
    "# Set the device id to use GPU. To use CPU only, set it to -1.\n",
    "gpu = 0\n",
    "\n",
    "# Now create an agent that will interact with the environment.\n",
    "agent = pfrl.agents.DoubleDQN(\n",
    "    model,\n",
    "    optimizer,\n",
    "    replay_buffer,\n",
    "    gamma,\n",
    "    explorer,\n",
    "    minibatch_size=batch_size,\n",
    "    replay_start_size=batch_size,\n",
    "    update_interval=1,\n",
    "    target_update_interval=1000,\n",
    "    phi=phi,\n",
    "    gpu=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72dcd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stocknet.logger as lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38396e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-09 12:57:27.476374 start episodes\n",
      "statistics: [('average_q', -0.0063182074), ('average_loss', 0.00015820811412323998), ('cumulative_steps', 10000), ('n_updates', 9999), ('rlen', 2)] R: -0.0012459722706180524 Mean R: 0.00021950411447532937 PL: -0.6449722706180783 Mean PL: 0.5290041144753187\n",
      "consumed time: 0:00:36.211206, may end on :2022-04-09 22:49:38.081821\n",
      "statistics: [('average_q', -0.07034607), ('average_loss', 0.03429108972026346), ('cumulative_steps', 20000), ('n_updates', 19999), ('rlen', 2)] R: -0.0009424651942673742 Mean R: -0.0009292130255372219 PL: -0.0284651942673734 Mean PL: -0.050713025537221124\n",
      "consumed time: 0:00:36.837387, may end on :2022-04-09 22:47:06.524317\n",
      "statistics: [('average_q', -0.107270755), ('average_loss', 0.06716067193352378), ('cumulative_steps', 30000), ('n_updates', 29999), ('rlen', 2)] R: -0.0009338362112315788 Mean R: -0.0009381361457170194 PL: -0.02583621123157808 Mean PL: -0.035936145717018676\n",
      "consumed time: 0:00:40.056035, may end on :2022-04-09 23:44:11.441142\n",
      "statistics: [('average_q', -0.1620722), ('average_loss', 0.008148745381326898), ('cumulative_steps', 40000), ('n_updates', 39999), ('rlen', 2)] R: 0.0003933797761049465 Mean R: -0.0008061561921599882 PL: 1.255379776104916 Mean PL: 0.0904438078400094\n",
      "consumed time: 0:00:35.054053, may end on :2022-04-09 23:53:30.677195\n",
      "statistics: [('average_q', -0.11135301), ('average_loss', 0.03726473283467216), ('cumulative_steps', 50000), ('n_updates', 49999), ('rlen', 2)] R: 0.00019854092788839438 Mean R: -0.0003949062413166053 PL: 1.114540927888386 Mean PL: 0.5065937586833984\n",
      "consumed time: 0:00:31.378114, may end on :2022-04-09 23:03:12.960346\n",
      "statistics: [('average_q', -0.08724572), ('average_loss', 0.02816107591964123), ('cumulative_steps', 60000), ('n_updates', 59999), ('rlen', 2)] R: -6.494896652963676e-05 Mean R: -9.639176447882492e-05 PL: 0.854051033470364 Mean PL: 0.8029082355211319\n",
      "consumed time: 0:00:31.518242, may end on :2022-04-09 22:19:53.539330\n",
      "statistics: [('average_q', -0.08143744), ('average_loss', 0.03417437349481187), ('cumulative_steps', 70000), ('n_updates', 69999), ('rlen', 2)] R: -0.000904847436289424 Mean R: -0.0005906468075686471 PL: -0.011847436289423318 Mean PL: 0.3060531924313552\n",
      "consumed time: 0:00:35.378689, may end on :2022-04-09 22:10:15.873011\n",
      "statistics: [('average_q', -0.08616699), ('average_loss', 0.04889329975552455), ('cumulative_steps', 80000), ('n_updates', 79999), ('rlen', 2)] R: 0.003339221366495117 Mean R: -0.00038109556971219585 PL: 4.228221366495128 Mean PL: 0.5130044302877983\n",
      "consumed time: 0:00:31.395546, may end on :2022-04-09 22:11:59.129457\n",
      "statistics: [('average_q', -0.061368935), ('average_loss', 0.02431428226112367), ('cumulative_steps', 90000), ('n_updates', 89999), ('rlen', 2)] R: -0.0014841106840248572 Mean R: -0.0008082728382852704 PL: -0.5681106840249085 Mean PL: 0.09772716171469677\n",
      "consumed time: 0:00:31.950291, may end on :2022-04-09 22:31:29.621956\n",
      "invalid valu in observations at 160568: -1.638390594269339\n",
      "statistics: [('average_q', -0.99999696), ('average_loss', 0.14478264444507657), ('cumulative_steps', 98196), ('n_updates', 98195), ('rlen', 2)] R: 0.18274937757873552 Mean R: 0.01710556074385696 PL: 14.141702073562598 Mean PL: 1.797183581791895\n",
      "consumed time: 0:00:02.284713, may end on :2022-04-09 20:38:59.193859\n",
      "statistics: [('average_q', -0.99992955), ('average_loss', 0.004878905788064003), ('cumulative_steps', 108196), ('n_updates', 108195), ('rlen', 2)] R: -0.0014746831489879488 Mean R: -0.0006983110890649635 PL: -0.5886831489879939 Mean PL: 0.20218891093498553\n",
      "consumed time: 0:00:30.216808, may end on :2022-04-09 21:48:09.746399\n",
      "statistics: [('average_q', -0.40696916), ('average_loss', 0.010642890124410949), ('cumulative_steps', 118196), ('n_updates', 118195), ('rlen', 2)] R: 0.01141427654000764 Mean R: 0.0015479934741431498 PL: 12.328276540007144 Mean PL: 2.4457934741431306\n",
      "consumed time: 0:00:32.994411, may end on :2022-04-09 21:56:12.709499\n",
      "statistics: [('average_q', -0.18439199), ('average_loss', 0.007518706664170021), ('cumulative_steps', 128196), ('n_updates', 128195), ('rlen', 2)] R: 0.00015852112345096252 Mean R: -0.0005911603457990661 PL: 1.0685211234509473 Mean PL: 0.30843965420091274\n",
      "consumed time: 0:00:32.638261, may end on :2022-04-09 22:04:45.580476\n",
      "statistics: [('average_q', -0.13019647), ('average_loss', 0.022297423242955743), ('cumulative_steps', 137781), ('n_updates', 137780), ('rlen', 2)] R: -2.510907500980677e-05 Mean R: -0.0006043282950710083 PL: 0.8728909249901942 Mean PL: 0.36310164887377083\n",
      "consumed time: 0:00:35.137137, may end on :2022-04-09 22:04:15.722759\n",
      "statistics: [('average_q', -0.09857553), ('average_loss', 0.04351156297279385), ('cumulative_steps', 147143), ('n_updates', 147142), ('rlen', 2)] R: -0.0012623834911604241 Mean R: -0.0008647395602078081 PL: -0.3653834911604515 Mean PL: 0.21599712789549114\n",
      "consumed time: 0:00:30.386728, may end on :2022-04-09 21:29:30.373774\n",
      "statistics: [('average_q', -0.09983372), ('average_loss', 0.03339430265056496), ('cumulative_steps', 156097), ('n_updates', 156096), ('rlen', 2)] R: -0.0009669661761703026 Mean R: 0.0023121929723144323 PL: -0.06296617617030206 Mean PL: 2.279781007994133\n",
      "consumed time: 0:00:33.249206, may end on :2022-04-09 21:03:36.213451\n",
      "statistics: [('average_q', -0.09056798), ('average_loss', 0.023967068983956653), ('cumulative_steps', 166097), ('n_updates', 166096), ('rlen', 2)] R: -0.0007258916484957634 Mean R: -0.0007924049331641796 PL: 0.18610835150423738 Mean PL: 0.10999506683580249\n",
      "consumed time: 0:00:32.507697, may end on :2022-04-09 22:12:52.158084\n",
      "statistics: [('average_q', -0.07315602), ('average_loss', 0.02919845135564363), ('cumulative_steps', 175557), ('n_updates', 175556), ('rlen', 2)] R: -0.0014993429319551123 Mean R: -0.0006647152438382211 PL: -0.6103429319551689 Mean PL: 0.3582452234054977\n",
      "consumed time: 0:00:38.341600, may end on :2022-04-09 22:27:46.197300\n",
      "statistics: [('average_q', -0.060402274), ('average_loss', 0.03440264256368024), ('cumulative_steps', 185557), ('n_updates', 185556), ('rlen', 2)] R: -0.0009411333156978385 Mean R: 0.0003174908916206753 PL: -0.03813331569783752 Mean PL: 1.2178908916206659\n",
      "consumed time: 0:00:37.464243, may end on :2022-04-09 23:10:55.962719\n",
      "statistics: [('average_q', -0.09944265), ('average_loss', 0.04893972452713143), ('cumulative_steps', 195387), ('n_updates', 195386), ('rlen', 2)] R: 0.003983079660613811 Mean R: 0.00010070215975067674 PL: 4.8780796606140715 Mean PL: 1.0220771166260625\n",
      "consumed time: 0:00:32.490373, may end on :2022-04-09 22:44:08.073431\n",
      "statistics: [('average_q', -0.084002316), ('average_loss', 0.02944635121861404), ('cumulative_steps', 205387), ('n_updates', 205386), ('rlen', 2)] R: 0.011684497211712263 Mean R: 0.0007318495234133537 PL: 12.590497211711764 Mean PL: 1.6394495234133082\n",
      "consumed time: 0:00:32.797942, may end on :2022-04-09 21:56:42.491976\n",
      "statistics: [('average_q', -0.09875813), ('average_loss', 0.048714782237356105), ('cumulative_steps', 215387), ('n_updates', 215386), ('rlen', 2)] R: -0.0011433707770779524 Mean R: -0.0001926147694587042 PL: -0.2683707770779669 Mean PL: 0.7050852305412698\n",
      "consumed time: 0:00:30.599880, may end on :2022-04-09 21:56:19.641164\n",
      "statistics: [('average_q', -0.13018158), ('average_loss', 0.027235561054764212), ('cumulative_steps', 225387), ('n_updates', 225386), ('rlen', 2)] R: -8.372878585778887e-05 Mean R: -0.0004330129794155653 PL: 0.8032712141422117 Mean PL: 0.46048702058439217\n",
      "consumed time: 0:00:33.578027, may end on :2022-04-09 21:52:11.916718\n",
      "statistics: [('average_q', -0.096365154), ('average_loss', 0.051826214876404854), ('cumulative_steps', 234261), ('n_updates', 234260), ('rlen', 2)] R: -0.0016913965305572765 Mean R: 0.0013869353031597431 PL: -0.7673965305573507 Mean PL: 1.4290239472909954\n",
      "consumed time: 0:00:30.512623, may end on :2022-04-09 21:04:01.571347\n",
      "statistics: [('average_q', -0.07042294), ('average_loss', 0.026430328972019263), ('cumulative_steps', 244261), ('n_updates', 244260), ('rlen', 2)] R: -0.0008862266321104875 Mean R: 0.0005003906836753425 PL: 0.018773367889513195 Mean PL: 1.404690683675263\n",
      "consumed time: 0:00:33.179129, may end on :2022-04-09 21:54:50.035096\n",
      "statistics: [('average_q', -0.59721), ('average_loss', 0.007404307454618291), ('cumulative_steps', 254261), ('n_updates', 254260), ('rlen', 2)] R: 0.0005889434823840191 Mean R: 2.2061250381569686e-05 PL: 1.4939434823839683 Mean PL: 0.9277612503815563\n",
      "consumed time: 0:00:31.629109, may end on :2022-04-09 21:47:51.373140\n",
      "statistics: [('average_q', -0.25773126), ('average_loss', 0.012976119101268972), ('cumulative_steps', 264261), ('n_updates', 264260), ('rlen', 2)] R: -0.0012191125410067484 Mean R: -0.0008531482830473723 PL: -0.3091125410067705 Mean PL: 0.05525171695261692\n",
      "consumed time: 0:00:36.608882, may end on :2022-04-09 22:20:17.895549\n",
      "invalid valu in observations at 191527: -1.5448173356032264\n",
      "statistics: [('average_q', -0.13817838), ('average_loss', 0.010571557533350902), ('cumulative_steps', 273380), ('n_updates', 273379), ('rlen', 2)] R: -0.000978959126372712 Mean R: 0.001994979246479637 PL: -0.07095912637271112 Mean PL: 0.29455030480411587\n",
      "consumed time: 0:00:31.025853, may end on :2022-04-09 21:29:48.248751\n",
      "statistics: [('average_q', -0.0011317113), ('average_loss', 0.02288458469120481), ('cumulative_steps', 283380), ('n_updates', 283379), ('rlen', 2)] R: -0.0011648499179811006 Mean R: 0.0005865775500441221 PL: -0.27484991798112646 Mean PL: 1.4850775500441453\n",
      "consumed time: 0:00:29.908063, may end on :2022-04-09 21:56:35.910746\n",
      "statistics: [('average_q', -0.6391494), ('average_loss', 0.004140552512039691), ('cumulative_steps', 293355), ('n_updates', 293354), ('rlen', 2)] R: -0.0009164497385473809 Mean R: 9.143107715153977e-05 PL: -0.009449738547380064 Mean PL: 0.9953987426294544\n",
      "consumed time: 0:00:33.049437, may end on :2022-04-09 21:58:00.515294\n",
      "statistics: [('average_q', -0.28948456), ('average_loss', 0.022056723444641803), ('cumulative_steps', 303355), ('n_updates', 303354), ('rlen', 2)] R: -0.0009156308662910795 Mean R: -0.0009472195445741008 PL: -0.02963086629107886 Mean PL: -0.04761954457409999\n",
      "consumed time: 0:00:31.555852, may end on :2022-04-09 21:56:45.268274\n",
      "statistics: [('average_q', -0.15723865), ('average_loss', 0.03835162066382337), ('cumulative_steps', 313355), ('n_updates', 313354), ('rlen', 2)] R: -0.0009374123481815256 Mean R: -0.0009422066503519249 PL: -0.03841234818152498 Mean PL: -0.04120665035192407\n",
      "consumed time: 0:00:32.393769, may end on :2022-04-09 22:02:45.101665\n",
      "statistics: [('average_q', -0.083459154), ('average_loss', 0.06405713156453674), ('cumulative_steps', 323355), ('n_updates', 323354), ('rlen', 2)] R: -0.000947505970130827 Mean R: -0.0009434929408698375 PL: -0.057505970130826194 Mean PL: -0.046292940869836624\n",
      "consumed time: 0:00:30.339730, may end on :2022-04-09 21:53:58.668598\n",
      "statistics: [('average_q', -0.08148271), ('average_loss', 0.029578946687397325), ('cumulative_steps', 333355), ('n_updates', 333354), ('rlen', 2)] R: -0.0009824215801613168 Mean R: -0.0009335401885001355 PL: -0.08342158016131598 Mean PL: -0.034340188500134664\n",
      "consumed time: 0:00:29.687564, may end on :2022-04-09 21:57:38.743648\n",
      "statistics: [('average_q', -0.07419327), ('average_loss', 0.04413842259022381), ('cumulative_steps', 343355), ('n_updates', 343354), ('rlen', 2)] R: -0.0009418441467930221 Mean R: -0.0009506035174943398 PL: -0.025844146793021328 Mean PL: -0.04960351749433929\n",
      "consumed time: 0:00:33.877843, may end on :2022-04-09 21:49:32.872714\n",
      "statistics: [('average_q', -0.84544444), ('average_loss', 0.0006288102172510613), ('cumulative_steps', 353355), ('n_updates', 353354), ('rlen', 2)] R: -0.0009414057102928145 Mean R: -0.0009622806277429961 PL: -0.012405710292813597 Mean PL: -0.059180627742995914\n",
      "consumed time: 0:00:29.913743, may end on :2022-04-09 21:58:08.404813\n",
      "statistics: [('average_q', -0.34026328), ('average_loss', 0.02046104239319776), ('cumulative_steps', 363355), ('n_updates', 363354), ('rlen', 2)] R: -0.0009611045995117217 Mean R: -0.0009467655555619247 PL: -0.06710459951172089 Mean PL: -0.04216555556192379\n",
      "consumed time: 0:00:31.448539, may end on :2022-04-09 21:58:01.253370\n",
      "statistics: [('average_q', -0.15921308), ('average_loss', 0.020102406498972217), ('cumulative_steps', 373355), ('n_updates', 373354), ('rlen', 2)] R: -0.0009420492384090273 Mean R: -0.0009497390935235944 PL: -0.02004923840902626 Mean PL: -0.04363909352359359\n",
      "consumed time: 0:00:33.057327, may end on :2022-04-09 21:55:01.217934\n",
      "statistics: [('average_q', -0.5967256), ('average_loss', 0.0045556581606195756), ('cumulative_steps', 383355), ('n_updates', 383354), ('rlen', 2)] R: -0.0009327909301544588 Mean R: -0.0007460369079271135 PL: -0.03579093015445825 Mean PL: -0.05493690792711401\n",
      "consumed time: 0:00:34.537099, may end on :2022-04-09 21:58:18.265283\n",
      "statistics: [('average_q', -0.27381873), ('average_loss', 0.018492877073254012), ('cumulative_steps', 393355), ('n_updates', 393354), ('rlen', 2)] R: -0.0009180735252980938 Mean R: -0.0009488464702171523 PL: -0.03407352529809321 Mean PL: -0.047946470217151906\n",
      "consumed time: 0:00:33.447820, may end on :2022-04-09 22:00:46.523354\n",
      "statistics: [('average_q', -0.15281506), ('average_loss', 0.021076243109629692), ('cumulative_steps', 403355), ('n_updates', 403354), ('rlen', 2)] R: -0.0009275368601712607 Mean R: -0.0009392209069667915 PL: -0.03353686017125985 Mean PL: -0.0318209069667909\n",
      "consumed time: 0:00:32.352793, may end on :2022-04-09 21:56:17.766379\n",
      "statistics: [('average_q', -0.0882825), ('average_loss', 0.023298015869356023), ('cumulative_steps', 413355), ('n_updates', 413354), ('rlen', 2)] R: -0.0009523337102995769 Mean R: -0.0009523514319953359 PL: -0.030333710299576253 Mean PL: -0.04185143199533537\n",
      "consumed time: 0:00:32.867195, may end on :2022-04-09 22:01:30.015096\n",
      "statistics: [('average_q', -0.100104414), ('average_loss', 0.059446569087521564), ('cumulative_steps', 423355), ('n_updates', 423354), ('rlen', 2)] R: -0.0009382657959543779 Mean R: -0.0009596973282902899 PL: -0.045265795954377325 Mean PL: -0.058897328290289555\n",
      "consumed time: 0:00:33.370294, may end on :2022-04-09 21:58:06.979479\n",
      "statistics: [('average_q', -0.099511474), ('average_loss', 0.03361916549194817), ('cumulative_steps', 433355), ('n_updates', 433354), ('rlen', 2)] R: -0.0009680578104263142 Mean R: -0.0009600774913970749 PL: -0.06805781042631347 Mean PL: -0.05567749139707413\n",
      "consumed time: 0:00:31.917344, may end on :2022-04-09 21:59:20.549016\n",
      "statistics: [('average_q', -0.06969984), ('average_loss', 0.03417073109572357), ('cumulative_steps', 443355), ('n_updates', 443354), ('rlen', 2)] R: -0.0009139320166362469 Mean R: -0.0009624627368853545 PL: -0.007932016636246294 Mean PL: -0.05986273688535413\n",
      "consumed time: 0:00:32.079052, may end on :2022-04-09 21:58:50.352682\n",
      "statistics: [('average_q', -0.09008061), ('average_loss', 0.061522449247351924), ('cumulative_steps', 453355), ('n_updates', 453354), ('rlen', 2)] R: -0.0009230619301535443 Mean R: -0.0009405166425683802 PL: -0.0370619301535439 Mean PL: -0.03181664256837954\n",
      "consumed time: 0:00:32.386898, may end on :2022-04-09 21:56:52.313071\n",
      "statistics: [('average_q', -0.045412693), ('average_loss', 0.009824850199389081), ('cumulative_steps', 463355), ('n_updates', 463354), ('rlen', 2)] R: -0.0009476877511265234 Mean R: -0.0009460440403676317 PL: -0.00568775112652266 Mean PL: -0.03704404036763094\n",
      "consumed time: 0:00:32.138422, may end on :2022-04-09 21:59:25.668392\n",
      "statistics: [('average_q', -0.09375872), ('average_loss', 0.03607405176285708), ('cumulative_steps', 473355), ('n_updates', 473354), ('rlen', 2)] R: -0.0009322110580992288 Mean R: -0.0009339368634426925 PL: -0.04621105809922817 Mean PL: -0.03603686344269183\n",
      "consumed time: 0:00:31.724761, may end on :2022-04-09 21:58:00.758492\n",
      "statistics: [('average_q', -0.08401524), ('average_loss', 0.02442404829740076), ('cumulative_steps', 483355), ('n_updates', 483354), ('rlen', 2)] R: -0.0010195402045793114 Mean R: -0.000952802521745126 PL: -0.1155402045793129 Mean PL: -0.0492025217451257\n",
      "consumed time: 0:00:32.548746, may end on :2022-04-09 21:56:27.643366\n",
      "statistics: [('average_q', -0.051238026), ('average_loss', 0.03206815989913188), ('cumulative_steps', 493355), ('n_updates', 493354), ('rlen', 2)] R: -0.0009494252009957664 Mean R: -0.0009646551185818316 PL: -0.02342520099576563 Mean PL: -0.06255511858183146\n",
      "consumed time: 0:00:31.981810, may end on :2022-04-09 21:55:55.587634\n",
      "statistics: [('average_q', -0.0595029), ('average_loss', 0.029578189171483506), ('cumulative_steps', 503355), ('n_updates', 503354), ('rlen', 2)] R: -0.0009326901708707002 Mean R: -0.0009499353241846905 PL: -0.025690170870699348 Mean PL: -0.047335324184690386\n",
      "consumed time: 0:00:34.221723, may end on :2022-04-09 21:58:24.296534\n",
      "statistics: [('average_q', -0.7685351), ('average_loss', 0.0031223550252714462), ('cumulative_steps', 513355), ('n_updates', 513354), ('rlen', 2)] R: 0.0029137833074425754 Mean R: -0.00016137101995775913 PL: 2.9377833074425728 Mean PL: 0.43052898004224077\n",
      "consumed time: 0:00:38.958989, may end on :2022-04-09 22:14:18.656877\n",
      "statistics: [('average_q', -0.30804333), ('average_loss', 0.01664918810134939), ('cumulative_steps', 523355), ('n_updates', 523354), ('rlen', 2)] R: -0.00013614713597195056 Mean R: 0.0010174631945781825 PL: -0.09914713597195052 Mean PL: 1.055763194578181\n",
      "consumed time: 0:00:39.144180, may end on :2022-04-09 22:48:12.929269\n",
      "statistics: [('average_q', -0.15691836), ('average_loss', 0.02042031765296942), ('cumulative_steps', 532584), ('n_updates', 532583), ('rlen', 2)] R: -2.079303855727826e-05 Mean R: 0.0037532350699988754 PL: 0.015206961442721755 Mean PL: 1.1877281388606955\n",
      "consumed time: 0:00:30.749122, may end on :2022-04-09 22:01:25.358809\n",
      "statistics: [('average_q', -0.10099195), ('average_loss', 0.03716773182154148), ('cumulative_steps', 542584), ('n_updates', 542583), ('rlen', 2)] R: -9.456095972391988e-06 Mean R: 0.0004328972990549542 PL: 0.02554390402760804 Mean PL: 0.46639729905495536\n",
      "consumed time: 0:00:30.487417, may end on :2022-04-09 22:13:56.481364\n",
      "statistics: [('average_q', -0.08247026), ('average_loss', 0.053523537215341845), ('cumulative_steps', 552584), ('n_updates', 552583), ('rlen', 2)] R: 0.0014741595287170758 Mean R: 9.105765035013452e-05 PL: 1.512159528717072 Mean PL: 0.1258576503501338\n",
      "consumed time: 0:00:35.782378, may end on :2022-04-09 22:08:08.847586\n",
      "statistics: [('average_q', -0.07999032), ('average_loss', 0.03168226555569646), ('cumulative_steps', 562584), ('n_updates', 562583), ('rlen', 2)] R: -0.0003969426446505969 Mean R: -0.00019722182902403765 PL: -0.3699426446505969 Mean PL: -0.16442182902403765\n",
      "consumed time: 0:01:09.178686, may end on :2022-04-09 23:41:20.659325\n",
      "statistics: [('average_q', 0.063564874), ('average_loss', 0.05108913838863382), ('cumulative_steps', 572584), ('n_updates', 572583), ('rlen', 2)] R: 0.0007293359467557482 Mean R: -0.00017633720393654142 PL: 0.7563359467557482 Mean PL: -0.1409372039365414\n",
      "consumed time: 0:00:37.686446, may end on :2022-04-09 23:09:24.238609\n",
      "statistics: [('average_q', -0.0044969292), ('average_loss', 0.04465665578842165), ('cumulative_steps', 582584), ('n_updates', 582583), ('rlen', 2)] R: -0.00047549230075648097 Mean R: 0.0011075364311849555 PL: -0.43949230075648094 Mean PL: 1.1399364311849574\n",
      "consumed time: 0:00:29.567065, may end on :2022-04-09 22:03:32.159310\n",
      "statistics: [('average_q', -0.03182038), ('average_loss', 0.015702658067675988), ('cumulative_steps', 592584), ('n_updates', 592583), ('rlen', 2)] R: -5.071669005723294e-05 Mean R: -0.00011548994622506418 PL: -0.014716690057232917 Mean PL: -0.0776899462250642\n",
      "consumed time: 0:00:34.850153, may end on :2022-04-09 22:15:09.788142\n",
      "statistics: [('average_q', -0.05698017), ('average_loss', 0.047260830402374276), ('cumulative_steps', 602584), ('n_updates', 602583), ('rlen', 2)] R: -0.0005606320753717458 Mean R: -7.323961921469808e-05 PL: -0.5276320753717457 Mean PL: -0.04023961921469844\n",
      "consumed time: 0:00:34.990722, may end on :2022-04-09 22:17:48.547644\n",
      "statistics: [('average_q', -0.27976006), ('average_loss', 0.023535892218351364), ('cumulative_steps', 612584), ('n_updates', 612583), ('rlen', 2)] R: 0.0034975808777192668 Mean R: 0.000211599128818711 PL: 3.531580877719263 Mean PL: 0.24519912881871062\n",
      "consumed time: 0:00:35.150850, may end on :2022-04-09 22:24:27.261891\n",
      "statistics: [('average_q', -0.62898976), ('average_loss', 0.007512669899332973), ('cumulative_steps', 622584), ('n_updates', 622583), ('rlen', 2)] R: -0.0008897752830710165 Mean R: -0.0009954630288254982 PL: -0.0057752830710157185 Mean PL: -0.14046302882550238\n",
      "consumed time: 0:00:36.623717, may end on :2022-04-09 22:29:49.737098\n",
      "statistics: [('average_q', -0.8307616), ('average_loss', 0.005660678767599165), ('cumulative_steps', 632584), ('n_updates', 632583), ('rlen', 2)] R: -0.0009075417725487497 Mean R: -0.0009302013561788884 PL: -0.042541772548748795 Mean PL: -0.03450135617888754\n",
      "consumed time: 0:00:35.025443, may end on :2022-04-09 22:27:01.998394\n",
      "statistics: [('average_q', -0.7233541), ('average_loss', 0.00256443874136858), ('cumulative_steps', 642584), ('n_updates', 642583), ('rlen', 2)] R: -0.0018084196889217457 Mean R: 0.0004712445596301832 PL: -0.9234196889218208 Mean PL: 1.3651445596301115\n",
      "consumed time: 0:00:35.725502, may end on :2022-04-09 22:31:19.422216\n",
      "statistics: [('average_q', -0.45232525), ('average_loss', 0.015609962460346134), ('cumulative_steps', 652584), ('n_updates', 652583), ('rlen', 2)] R: -0.001768157069935043 Mean R: 0.000411939679582733 PL: -0.8761570699351191 Mean PL: 1.3132396795826504\n",
      "consumed time: 0:00:36.299033, may end on :2022-04-09 22:28:26.657659\n",
      "statistics: [('average_q', -0.2131073), ('average_loss', 0.01406258676644792), ('cumulative_steps', 662584), ('n_updates', 662583), ('rlen', 2)] R: -0.001704278457978564 Mean R: -0.0005077616459347094 PL: -0.8112784579786467 Mean PL: 0.3957383540652515\n",
      "consumed time: 0:00:34.372359, may end on :2022-04-09 22:29:18.276457\n",
      "statistics: [('average_q', -0.14091234), ('average_loss', 0.012303623397724377), ('cumulative_steps', 672584), ('n_updates', 672583), ('rlen', 2)] R: -0.0013883002909989592 Mean R: -0.00047039254121086554 PL: -0.4823002909990038 Mean PL: 0.42920745878911004\n",
      "consumed time: 0:00:36.708953, may end on :2022-04-09 22:30:38.925386\n",
      "statistics: [('average_q', -0.10698294), ('average_loss', 0.018528718764623805), ('cumulative_steps', 682584), ('n_updates', 682583), ('rlen', 2)] R: -0.00029658790747025667 Mean R: -2.5206561062931685e-05 PL: 0.5944120925297444 Mean PL: 0.8746934389370832\n",
      "consumed time: 0:00:35.316248, may end on :2022-04-09 22:37:43.891804\n",
      "statistics: [('average_q', -0.10118492), ('average_loss', 0.018000730121206544), ('cumulative_steps', 692584), ('n_updates', 692583), ('rlen', 2)] R: 0.002913376167134581 Mean R: -0.0003558698340759411 PL: 3.816376167134483 Mean PL: 0.5448301659240267\n",
      "consumed time: 0:00:33.093150, may end on :2022-04-09 22:09:56.106495\n",
      "statistics: [('average_q', -0.07903489), ('average_loss', 0.03366314848102182), ('cumulative_steps', 702556), ('n_updates', 702555), ('rlen', 2)] R: -0.0005786414156628151 Mean R: -0.0009924242145287795 PL: 0.31135858433718555 Mean PL: -0.09189243721325228\n",
      "consumed time: 0:00:37.391003, may end on :2022-04-09 22:35:29.442202\n",
      "statistics: [('average_q', -0.99999547), ('average_loss', 0.004898997964337468), ('cumulative_steps', 712556), ('n_updates', 712555), ('rlen', 2)] R: -0.0009317765474511788 Mean R: 0.0001567637148184473 PL: -0.03277654745117822 Mean PL: 1.0532637148183783\n",
      "consumed time: 0:00:37.396984, may end on :2022-04-09 22:34:41.273983\n",
      "statistics: [('average_q', -0.6269339), ('average_loss', 0.00498973886877124), ('cumulative_steps', 722556), ('n_updates', 722555), ('rlen', 2)] R: -0.0006562529738104244 Mean R: 0.00010457005758729681 PL: 0.24374702618957642 Mean PL: 1.0070700575873275\n",
      "consumed time: 0:00:37.112014, may end on :2022-04-09 22:37:52.398318\n",
      "statistics: [('average_q', -0.25306207), ('average_loss', 0.02134051917247385), ('cumulative_steps', 732556), ('n_updates', 732555), ('rlen', 2)] R: -0.000733641337006992 Mean R: 0.000925733959903504 PL: 0.1763586629930088 Mean PL: 1.832233959903546\n",
      "consumed time: 0:00:36.799080, may end on :2022-04-09 22:38:33.973044\n",
      "statistics: [('average_q', -0.14846164), ('average_loss', 0.03549770498951141), ('cumulative_steps', 742556), ('n_updates', 742555), ('rlen', 2)] R: 0.0029989769361073114 Mean R: 0.00020032723574448368 PL: 3.906976936107218 Mean PL: 1.1047272357444737\n",
      "consumed time: 0:00:37.826026, may end on :2022-04-09 22:36:30.534665\n",
      "statistics: [('average_q', -0.10885727), ('average_loss', 0.035366532354262785), ('cumulative_steps', 752556), ('n_updates', 752555), ('rlen', 2)] R: -0.0009326214377477535 Mean R: -0.0006349968781975249 PL: -0.0386214377477529 Mean PL: 0.2585031218024711\n",
      "consumed time: 0:00:37.497958, may end on :2022-04-09 22:36:16.133433\n",
      "statistics: [('average_q', -0.09693991), ('average_loss', 0.05724723367671451), ('cumulative_steps', 762556), ('n_updates', 762555), ('rlen', 2)] R: 0.00029173112968428386 Mean R: 0.0012031113055376184 PL: 1.1707311296842575 Mean PL: 2.107611305537723\n",
      "consumed time: 0:00:37.018998, may end on :2022-04-09 22:35:28.449690\n",
      "statistics: [('average_q', -0.078164965), ('average_loss', 0.04896852505603455), ('cumulative_steps', 772556), ('n_updates', 772555), ('rlen', 2)] R: 0.0007606552043514149 Mean R: 0.0008290153126135135 PL: 1.6686552043513427 Mean PL: 1.7352153126134673\n",
      "consumed time: 0:00:38.016977, may end on :2022-04-09 22:38:16.465047\n",
      "statistics: [('average_q', -0.07697148), ('average_loss', 0.031143777649208566), ('cumulative_steps', 782122), ('n_updates', 782121), ('rlen', 2)] R: -0.0027142992173469667 Mean R: -0.0008494540928314861 PL: -1.024293357018416 Mean PL: 0.12944649320134613\n",
      "consumed time: 0:00:20.133015, may end on :2022-04-09 22:30:58.708451\n",
      "statistics: [('average_q', -0.06715934), ('average_loss', 0.029798158431549142), ('cumulative_steps', 792122), ('n_updates', 792121), ('rlen', 2)] R: -0.0009158679946589563 Mean R: -0.000582477309584817 PL: -0.013867994658955529 Mean PL: 0.3251226904151446\n",
      "consumed time: 0:00:37.450005, may end on :2022-04-09 22:36:31.918422\n",
      "statistics: [('average_q', -0.07157971), ('average_loss', 0.03975984636047859), ('cumulative_steps', 802122), ('n_updates', 802121), ('rlen', 2)] R: 3.9093927004802984e-05 Mean R: -0.00032614527560502043 PL: 0.9350939270048038 Mean PL: 0.5725547243949884\n",
      "consumed time: 0:00:31.171998, may end on :2022-04-09 22:21:02.845538\n",
      "statistics: [('average_q', -0.05919686), ('average_loss', 0.029671417376939357), ('cumulative_steps', 812122), ('n_updates', 812121), ('rlen', 2)] R: -0.0008454879930090807 Mean R: 0.0005643543416329563 PL: 0.0555120069909201 Mean PL: 1.460654341632976\n",
      "consumed time: 0:00:33.932003, may end on :2022-04-09 22:21:20.713107\n",
      "statistics: [('average_q', -0.07211316), ('average_loss', 0.03374371948802871), ('cumulative_steps', 822122), ('n_updates', 822121), ('rlen', 2)] R: -0.0014211329958899036 Mean R: -0.00023195964911083524 PL: -0.5021329958899472 Mean PL: 0.6657403508891526\n",
      "consumed time: 0:00:36.150005, may end on :2022-04-09 22:29:26.742262\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "max_step_len = 1000\n",
    "mr = 0\n",
    "pl = 0\n",
    "ep_consumed_total_time = datetime.timedelta(0)\n",
    "print(datetime.datetime.now(),'start episodes')\n",
    "\n",
    "logger = lg.pt_logs(env, folder='logs/bc_rl_5min_macd_budgets_v2')\n",
    "## show details\n",
    "##\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    #obs = obs.to('cpu').detach().numpy().copy()\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    logs = []\n",
    "    ep_start_time = datetime.datetime.now()\n",
    "    while True:\n",
    "        # Uncomment to watch the behavior in a GUI window\n",
    "        #env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, ops = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        reset = t == max_step_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        logger.store(obs, action, reward)\n",
    "        if reset or done:\n",
    "            break\n",
    "    ep_end_time = datetime.datetime.now()\n",
    "    ep_consumed_time = ep_end_time - ep_start_time\n",
    "    ep_consumed_total_time += ep_consumed_time\n",
    "    logger.save(i)\n",
    "    pl += env.pl\n",
    "    mr += R/t\n",
    "    if i % 10 == 0:\n",
    "        print('statistics:', agent.get_statistics(), 'R:', R/t, 'Mean R:', mr/10, 'PL:', env.pl, 'Mean PL:', pl/10)\n",
    "        print(f\"consumed time: {ep_consumed_time}, may end on :{ep_end_time + (n_episodes -i) *  ep_consumed_total_time/10}\")\n",
    "        ep_consumed_total_time = datetime.timedelta(0)\n",
    "        mr = 0\n",
    "        pl = 0\n",
    "print(f'Finished on {datetime.datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d82536",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bc_rl_5min_macd_50_with_budgets_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16078437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
